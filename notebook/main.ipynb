{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "The task is from [aidea](https://aidea-web.tw/topic/c4a666bb-7d83-45a6-8c3b-57514faf2901), the goal is to predict the sentiment of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv('../data/imdb.csv')\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(imdb_df, test_df, on='review', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID        29341\n",
       "review    29341\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8675914249684742"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ID'].count() / len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41411</td>\n",
       "      <td>I watched this film because I'm a big fan of R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37586</td>\n",
       "      <td>It does not seem that this movie managed to pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6017</td>\n",
       "      <td>Enough is not a bad movie , just mediocre .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44656</td>\n",
       "      <td>my friend and i rented this one a few nights a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38711</td>\n",
       "      <td>Just about everything in this movie is wrong, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>8019</td>\n",
       "      <td>It 's one of the most honest films ever made a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29337</th>\n",
       "      <td>453</td>\n",
       "      <td>An absorbing and unsettling psychological drama .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29338</th>\n",
       "      <td>13097</td>\n",
       "      <td>Soylent Green IS...a really good movie, actual...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>26896</td>\n",
       "      <td>There just isn't enough here. There a few funn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29340</th>\n",
       "      <td>27094</td>\n",
       "      <td>This show was absolutely terrible. For one Geo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29341 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                             review  sentiment\n",
       "0      41411  I watched this film because I'm a big fan of R...          0\n",
       "1      37586  It does not seem that this movie managed to pl...          1\n",
       "2       6017        Enough is not a bad movie , just mediocre .          0\n",
       "3      44656  my friend and i rented this one a few nights a...          0\n",
       "4      38711  Just about everything in this movie is wrong, ...          0\n",
       "...      ...                                                ...        ...\n",
       "29336   8019  It 's one of the most honest films ever made a...          1\n",
       "29337    453  An absorbing and unsettling psychological drama .          1\n",
       "29338  13097  Soylent Green IS...a really good movie, actual...          1\n",
       "29339  26896  There just isn't enough here. There a few funn...          0\n",
       "29340  27094  This show was absolutely terrible. For one Geo...          0\n",
       "\n",
       "[29341 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41411</td>\n",
       "      <td>I watched this film because I'm a big fan of R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37586</td>\n",
       "      <td>It does not seem that this movie managed to pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6017</td>\n",
       "      <td>Enough is not a bad movie , just mediocre .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44656</td>\n",
       "      <td>my friend and i rented this one a few nights a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38711</td>\n",
       "      <td>Just about everything in this movie is wrong, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                             review  sentiment\n",
       "0  41411  I watched this film because I'm a big fan of R...          0\n",
       "1  37586  It does not seem that this movie managed to pl...          1\n",
       "2   6017        Enough is not a bad movie , just mediocre .          0\n",
       "3  44656  my friend and i rented this one a few nights a...          0\n",
       "4  38711  Just about everything in this movie is wrong, ...          0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df['sentiment'] = imdb_df['sentiment'].replace(['negative', 'positive'], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['review', 'sentiment']], imdb_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hilarious, clean, light-hearted, and quote-worthy. What else can you ask for in a film? This is my all-time, number one favorite movie. Ever since I was a little girl, I've dreamed of owning a blue van with flames and an observation bubble.<br /><br />The cliché characters in ridiculous situations are what make this film such great fun. The wonderful comedic chemistry between Stephen Furst (Harold) and Andy Tennant (Melio) make up most of my favorite parts of the movie. And who didn't love the hopeless awkwardness of Flynch? Don't forget the airport antics of Leon's cronies, dressed up as Hari Krishnas: dancing, chanting and playing the tambourine--unbeatable! The clues are genius, the locations are classic, and the plot is timeless.<br /><br />A word to the wise, if you didn't watch this film when you were little, it probably won't win a place in your heart today. But nevertheless give it a chance, you may find that \"It doesn't matter what you say, it doesn't matter what you do, you've gotta play.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        7\n",
       "Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stimulating and captivating, always keeping the viewer peeking around the corner to see what was coming up next. She is as down to earth and as personable as you get, like one of us which made the show all the more enjoyable. Special guests, who are friends as well made for a nice surprise too. Loved the 'first' theme and that the audience was invited to play along too. I must admit I was shocked to see her come in under her time limits on a few things, but she did it and by golly I'll be writing those recipes down. Saving time in the kitchen means more time with family. Those who haven't tuned in yet, find out what channel and the time, I assure you that you won't be disappointed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      7\n",
       "How has this piece of crap stayed on TV this long? It's terrible. It makes me want to shoot someone. It's so fake that it is actually worse than a 1940s sci-fi movie. I'd rather have a stroke than watch this nonsense. I remember watching it when it first came out. I thought, hey this could be interesting, then I found out how absolutely, insanely, ridiculously stupid it really was. It was so bad that I actually took out my pocket knife and stuck my hand to the table.<br /><br />Please people, stop watching this and all other reality shows, they're the trash that is jamming the networks and canceling quality programming that requires some thought to create.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      6\n",
       "Nickelodeon has gone down the toilet. They have kids saying things like \"Oh my God!\" and \"We're screwed\"<br /><br />This show promotes hate for people who aren't good looking, or aren't in the in crowd. It say that sexual promiscuity is alright, by having girls slobbering over shirtless boys. Not to mention the overweight boy who takes off his shirt. The main characters basically shun anyone out of the ordinary. Carly's friend Sam, who may be a lesbian, beats the snot out of anybody that crosses her path, which says it's alright to be a b**ch. This show has so much negativity in it that nobody should watch it! I give it a 0 out of 10!!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          5\n",
       "This show comes up with interesting locations as fast as the travel channel. It is billed as reality but in actuality it is pure prime time soap opera. It's tries to use exotic locales as a facade to bring people into a phony contest & then proceeds to hook viewers on the contestants soap opera style.<br /><br />It also borrows from an early CBS game show pioneer- Beat The Clock- by inventing situations for its contestants to try & overcome. Then it rewards the winner money. If they can spice it up with a little interaction between the characters, even better. While the game format is in slow motion versus Beat The Clock- the real accomplishment of this series is to escape reality. <br /><br />This show has elements of several types of successful past programs. Reality television, hardly, but if your hooked on the contestants, locale or contest, this is your cup of tea. If your not, this entire series is as I say, drivel dripping with gravy. It is another show hiding behind the reality label which is the trend it started in 2000.<br /><br />It is slick & well produced, so it might last a while yet. After all, so do re-runs of Gilligan's Island, Green Acres, The Beverly Hillbillies & The Brady Bunch. This just doesn't employ professional actors. The intelligence level is about the same.                                                                                                                                                                                                                                                                                                                                                                                                                                                    5\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ..\n",
       "It was hard to watch this film and be totally fair and objective since I am a big fan the original 1944 movie. That, to me and many others, is one of the greatest film noirs ever made. Realizing this is simply a shortened made-for-TV film and that most people had trashed it, I didn't expect much, but you can't help but compare this with the '44 film. Scene after scene, I found myself comparing what I was looking at it, and remembering how it played out with Fred MacMurray, Barbara Stanwyck, Edward G. Robinson and others. Now I was seeing these famous actors playing their famous roles replaced by Richard Crenna, Samantha Eggar and Lee J. Cobb.<br /><br />When it was all over, I found it wasn't as bad as I had expected but it's no match for the 1944 original. The two main areas in which this made-for-TV film wasn't as good were (1) the electricity between the two leads was missing and (2) being only 90 minutes, they rushed the story with hardly time to develop the plot, characters and chemistry between those leads. Crenna and Eggar were flat, and simply no match for MacMurray and Stanwyck as \"Walter Neff\" and \"Phyllis Dietrichson,\" respectively.<br /><br />Where this re-make held its own was in the other characters, such as \"Barton Keyes\" and \"Edward Norton.\" Cobb was terrific as Keyes and Robert Webber as Norton, head of the insurance company. It also was somewhat interesting to see the time frame changed, so the houses, cars, telephones, dictating machines, etc., were all early '70s instead of mid '40s. Otherwise, the storyline was very similar, just rushed.<br /><br />However, one viewing was enough and I will happily go back to the original version for the rest of my viewings of this classic story and film.    1\n",
       "An extremely dark and brooding show with an excellent cast. One of the few shows that I try to watch on a regular basis. Glad to see Bebe Neuwirth in a recurring role, but feel Andre Braugher is underutilized. He is one intense actor! Hope CBS gives it a better time slot next season.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
       "I mistakenly thought that this neo-noir effort from the Buffalo - Niagara Falls area might be something different. Unfortunately I was incorrect. There are are many problems with \"The Falls\", that really have nothing to do with it's low budget video production. Immediately one has to question why all the constant narration? My feeling is that if you have a decent script, the audience will follow along, without having to be insulted with voice over storytelling. The acting is very amateurish, which is not unexpected, but simply adds to the problems. Finally, the entire thing is annoyingly shot like an MTV music video, which I found to be totally unacceptable. The narration, bad acting, and annoying video effects are all good reasons why this should be avoided. - MERK                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1\n",
       "Dolemite is a blaxploitation film about, well, Dolemite and his army of kung fu killer women, led by Queen Bee. He fights to get his club, The Total Experience, back from Willie Green by utilizing their kung fu abilities and their devotion to him. I liked this movie because of the witty dialogue and also the use of Rudy Ray Moore's ability to preach to his brothers in rhyme.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
       "What 's most refreshing about Real Women Have Curves is its unforced comedy-drama and its relaxed , natural-seeming actors .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
       "Name: review, Length: 53960, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df.drop_duplicates(subset=('clean_text')).reindex(columns=('review', 'clean_text', 'sentiment')).to_csv('../data/all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df = df.drop_duplicates(subset=('review'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df.to_csv('../data/all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I watched this film because I'm a big fan of R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It does not seem that this movie managed to pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enough is not a bad movie , just mediocre .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my friend and i rented this one a few nights a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just about everything in this movie is wrong, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I watched this film because I'm a big fan of R...          0\n",
       "1  It does not seem that this movie managed to pl...          1\n",
       "2        Enough is not a bad movie , just mediocre .          0\n",
       "3  my friend and i rented this one a few nights a...          0\n",
       "4  Just about everything in this movie is wrong, ...          0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    27354\n",
       "0    26606\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['len'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = list()\n",
    "\n",
    "for text in train_df['review']:\n",
    "    lens += [len(tokenizer(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['len'] = lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    957\n",
       "0    830\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['len'] > 600]['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29341.000000</td>\n",
       "      <td>29341.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29348.411097</td>\n",
       "      <td>0.509662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17002.074346</td>\n",
       "      <td>0.499915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14564.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29348.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44162.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>58681.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID     sentiment\n",
       "count  29341.000000  29341.000000\n",
       "mean   29348.411097      0.509662\n",
       "std    17002.074346      0.499915\n",
       "min        4.000000      0.000000\n",
       "25%    14564.000000      0.000000\n",
       "50%    29348.000000      1.000000\n",
       "75%    44162.000000      1.000000\n",
       "max    58681.000000      1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14954\n",
       "0    14387\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22622</td>\n",
       "      <td>Robert Lansing plays a scientist experimenting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10162</td>\n",
       "      <td>Well I've enjoy this movie, even though someti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17468</td>\n",
       "      <td>First things first - though I believe Joel Sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42579</td>\n",
       "      <td>I watched this movie on the grounds that Amber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701</td>\n",
       "      <td>A certain sexiness underlines even the dullest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>30370</td>\n",
       "      <td>It is difficult to rate a writer/director's fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29337</th>\n",
       "      <td>18654</td>\n",
       "      <td>After watching this movie once, it quickly bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29338</th>\n",
       "      <td>47985</td>\n",
       "      <td>Even though i sat and watched the whole thing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>9866</td>\n",
       "      <td>Warning Spoilers following. Superb recreation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29340</th>\n",
       "      <td>35559</td>\n",
       "      <td>My, my, my: Peter Cushing and Donald Pleasance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                             review\n",
       "0      22622  Robert Lansing plays a scientist experimenting...\n",
       "1      10162  Well I've enjoy this movie, even though someti...\n",
       "2      17468  First things first - though I believe Joel Sch...\n",
       "3      42579  I watched this movie on the grounds that Amber...\n",
       "4        701  A certain sexiness underlines even the dullest...\n",
       "...      ...                                                ...\n",
       "29336  30370  It is difficult to rate a writer/director's fi...\n",
       "29337  18654  After watching this movie once, it quickly bec...\n",
       "29338  47985  Even though i sat and watched the whole thing,...\n",
       "29339   9866  Warning Spoilers following. Superb recreation ...\n",
       "29340  35559  My, my, my: Peter Cushing and Donald Pleasance...\n",
       "\n",
       "[29341 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>30370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29337</th>\n",
       "      <td>18654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29338</th>\n",
       "      <td>47985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>9866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29340</th>\n",
       "      <td>35559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  sentiment\n",
       "0      22622          1\n",
       "1      10162          1\n",
       "2      17468          1\n",
       "3      42579          1\n",
       "4        701          1\n",
       "...      ...        ...\n",
       "29336  30370          1\n",
       "29337  18654          1\n",
       "29338  47985          1\n",
       "29339   9866          1\n",
       "29340  35559          1\n",
       "\n",
       "[29341 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=123) # Define the split - into 2 folds \n",
    "kf.get_n_splits(train_df) # returns the number of splitting iterations in the cross-validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41411</td>\n",
       "      <td>I watched this film because I'm a big fan of R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37586</td>\n",
       "      <td>It does not seem that this movie managed to pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6017</td>\n",
       "      <td>Enough is not a bad movie , just mediocre .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                             review  sentiment\n",
       "0  41411  I watched this film because I'm a big fan of R...          0\n",
       "1  37586  It does not seem that this movie managed to pl...          1\n",
       "2   6017        Enough is not a bad movie , just mediocre .          0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0 1 2 3 5 6 7 8 9] TEST: [4]\n",
      "9 1\n",
      "TRAIN: [1 2 3 4 5 6 7 8 9] TEST: [0]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 5 6 8 9] TEST: [7]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 6 7 8 9] TEST: [5]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 5 6 7 9] TEST: [8]\n",
      "9 1\n",
      "TRAIN: [0 1 2 4 5 6 7 8 9] TEST: [3]\n",
      "9 1\n",
      "TRAIN: [0 2 3 4 5 6 7 8 9] TEST: [1]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 5 7 8 9] TEST: [6]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 5 6 7 8] TEST: [9]\n",
      "9 1\n",
      "TRAIN: [0 1 3 4 5 6 7 8 9] TEST: [2]\n",
      "9 1\n"
     ]
    }
   ],
   "source": [
    "X, y = train_df, train_df['sentiment'].to_numpy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# Removing all punctuations from Text\n",
    "mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def word_replace(text):\n",
    "    return text.replace('<br />','')\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess(text):\n",
    "    text=clean_contractions(text,mapping)\n",
    "    text=text.lower()\n",
    "    text=word_replace(text)\n",
    "    text=remove_urls(text)\n",
    "    text=remove_html(text)\n",
    "#     text=remove_stopwords(text)\n",
    "    text=remove_punctuation(text)\n",
    "#     text=stem_words(text) ## Takes too much of time\n",
    "#     text=lemmatize_words(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/kr0zqp696h3bm6dmv7lmb_jc0000gn/T/ipykernel_8573/3308018844.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  drop_df['clean_text'] = drop_df['review'].apply(lambda x: preprocess(x))\n"
     ]
    }
   ],
   "source": [
    "drop_df['clean_text'] = drop_df['review'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'] = train_df['review'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['clean_text'] = test_df['review'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i watched this film because i am a big fan of ...\n",
       "1    it does not seem that this movie managed to pl...\n",
       "2            enough is not a bad movie  just mediocre \n",
       "3    my friend and i rented this one a few nights a...\n",
       "4    just about everything in this movie is wrong w...\n",
       "5    this is not bonnie and clyde or thelma and lou...\n",
       "6    i have to say that i really liked under siege ...\n",
       "7    kramer vs kramer is a nearheartening drama abo...\n",
       "8    like the other comments says this might be sur...\n",
       "9    the tunes are the best aspect of this televisi...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_df['clean_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_df['review']:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x14fd26d60>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab(Counter(dict(counter.most_common(10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102969"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 241, 0, 1]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[i] for i in ['I', 'am', 'aaaaaaaaaaaaa', '<pad>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(X):\n",
    "    if isinstance(X, list):\n",
    "        return [[vocab[i] for i in tokenizer(preprocess(text))] for text in X]\n",
    "    return [vocab[i] for i in tokenizer(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fp/kr0zqp696h3bm6dmv7lmb_jc0000gn/T/ipykernel_8573/1156103005.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I am a good boy! ADJISDAKSD unkqwjs <pad> <pad>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fp/kr0zqp696h3bm6dmv7lmb_jc0000gn/T/ipykernel_8573/3666183553.py\u001b[0m in \u001b[0;36mtext_pipeline\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fp/kr0zqp696h3bm6dmv7lmb_jc0000gn/T/ipykernel_8573/3666183553.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "text_pipeline(\"I am a good boy! ADJISDAKSD unkqwjs <pad> <pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 241, 5, 57], [412, 36]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline([\"I am a good\", \"boy!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Data Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch, use_bag=False):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_text, _label) in batch:\n",
    "         label_list.append(_label)\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    if use_bag:\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        text_list = torch.cat(text_list)\n",
    "    else:\n",
    "        offsets = torch.tensor(offsets[1:], dtype=torch.int64)\n",
    "        text_list = pad_sequence(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "df = train_df.iloc[:100]\n",
    "train_iter = list(zip(df['review'], df['sentiment']))\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8]) torch.Size([513, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([513, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 0, 0, 0, 1, 0, 1]), tensor([[  13,   11,  197,  ...,   14,   13, 5130],\n",
      "        [ 295,  130,   10,  ...,  213,   33, 1483],\n",
      "        [  14,   29,   29,  ...,    9,    8,    3],\n",
      "        ...,\n",
      "        [   0,  272,    0,  ...,    0,    0,    0],\n",
      "        [   0,  319,    0,  ...,    0,    0,    0],\n",
      "        [   0,    3,    0,  ...,    0,    0,    0]]), tensor([210, 513,  10, 233, 237, 276,  94, 309]))\n",
      "torch.Size([8]) torch.Size([537, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([537, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 0, 1, 1, 1, 0, 0]), tensor([[   45,     2,    14,  ...,    51,  4700,    18],\n",
      "        [    2,  3725,   373,  ...,    26,    12,  3034],\n",
      "        [   87,    31,   119,  ...,   531,   732,    18],\n",
      "        ...,\n",
      "        [    0,     0, 85389,  ...,     0,     0,     0],\n",
      "        [    0,     0,   373,  ...,     0,     0,     0],\n",
      "        [    0,     0,     3,  ...,     0,     0,     0]]), tensor([154, 153, 537,  13, 198, 185, 462,  14]))\n",
      "torch.Size([8]) torch.Size([394, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([394, 8]) torch.Size([8])\n",
      "(tensor([1, 1, 0, 0, 0, 0, 1, 1]), tensor([[  95,   14,   80,  ...,   12,  474,    5],\n",
      "        [  90, 7142,    8,  ...,    2,    3, 5631],\n",
      "        [  31, 5908, 1451,  ..., 1871, 4915, 5907],\n",
      "        ...,\n",
      "        [   0,    0,   25,  ...,    0,    0,    0],\n",
      "        [   0,    0,  136,  ...,    0,    0,    0],\n",
      "        [   0,    0,   24,  ...,    0,    0,    0]]), tensor([285, 157, 394, 296, 238, 190,  17,  12]))\n",
      "torch.Size([8]) torch.Size([221, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([221, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 0, 0, 1, 0, 0, 1]), tensor([[   14,     2,    47,  ...,  1842,    13,    11],\n",
      "        [   23,   349, 11118,  ...,     4,   241,     9],\n",
      "        [   10,    77,     9,  ...,  1328,   742,    16],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,    19,     0,     0],\n",
      "        [    0,     0,     0,  ...,    78,     0,     0],\n",
      "        [    0,     0,     0,  ...,     3,     0,     0]]), tensor([149, 180, 215,  15, 217, 221,  96, 123]))\n",
      "torch.Size([8]) torch.Size([1121, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([1121, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 1, 1, 0, 1, 1, 0]), tensor([[2020,   70,   99,  ...,   54,   13,   46],\n",
      "        [3173,  109,   65,  ...,   11,  416,   17],\n",
      "        [   4,   56,  818,  ..., 1435,  120,    5],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]]), tensor([ 614,  292,  295,   57, 1121,   12,  272,  139]))\n",
      "torch.Size([8]) torch.Size([324, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([324, 8]) torch.Size([8])\n",
      "(tensor([0, 0, 1, 1, 0, 0, 1, 1]), tensor([[   20,    37,     5,  ...,    37,   639,   301],\n",
      "        [   99,   220,   964,  ...,    54,    78,   650],\n",
      "        [ 2996,    11, 61405,  ...,   231,   377,    49],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,     0,     3,     0],\n",
      "        [    0,     0,     0,  ...,     0,     3,     0],\n",
      "        [    0,     0,     0,  ...,     0,     3,     0]]), tensor([ 23,  89,  16,  30,  76, 291, 324, 278]))\n",
      "torch.Size([8]) torch.Size([282, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([282, 8]) torch.Size([8])\n",
      "(tensor([1, 1, 1, 1, 0, 1, 1, 1]), tensor([[    2,  3011,    11,  ...,     2,    76,    19],\n",
      "        [16978,  5179,     9,  ...,  3022,  2178,    38],\n",
      "        [   10,  7939,    16,  ...,   209,     2,     2],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0]]), tensor([149, 147, 185, 193, 282,  21, 160,  31]))\n",
      "torch.Size([8]) torch.Size([492, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([492, 8]) torch.Size([8])\n",
      "(tensor([1, 0, 1, 1, 0, 1, 1, 1]), tensor([[  209,    11,    18,  ...,    46,  8437,   584],\n",
      "        [  100,  1893,  2445,  ...,     9,    10, 76474],\n",
      "        [    4,     3, 50867,  ...,    16,     5,     4],\n",
      "        ...,\n",
      "        [    0,     0,    99,  ...,     0,     0,     0],\n",
      "        [    0,     0,   123,  ...,     0,     0,     0],\n",
      "        [    0,     0,     3,  ...,     0,     0,     0]]), tensor([ 23,  45, 492,  18, 176,  18, 164, 350]))\n",
      "torch.Size([8]) torch.Size([941, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([941, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 0, 0, 0, 1, 0, 1]), tensor([[   14,  9276,    14,  ...,  5080,    54,    11],\n",
      "        [ 1188,   581,   332,  ...,  3805,  1487,     9],\n",
      "        [   66, 46576,     9,  ...,    25,    53,    16],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,     0,     0,   104],\n",
      "        [    0,     0,     0,  ...,     0,     0,    81],\n",
      "        [    0,     0,     0,  ...,     0,     0,     3]]), tensor([154, 325, 674, 136, 299, 750,  23, 941]))\n",
      "torch.Size([8]) torch.Size([461, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([461, 8]) torch.Size([8])\n",
      "(tensor([1, 0, 1, 0, 0, 0, 1, 1]), tensor([[   2,   14,   35,  ...,   13, 4580,  906],\n",
      "        [1965,   21,    7,  ...,  241, 6124,   55],\n",
      "        [   4,   71,    2,  ...,   29,    4,   45],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]]), tensor([157, 362, 135, 220, 461, 353, 148, 209]))\n",
      "torch.Size([8]) torch.Size([291, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([291, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 1, 1, 0, 0, 1, 0]), tensor([[  13,    2, 1824,  ...,    5,    9,  573],\n",
      "        [ 418, 1125,   13,  ...,  329,   39,  456],\n",
      "        [   9,  331,  758,  ...,  218,   38,    7],\n",
      "        ...,\n",
      "        [   0,   11,    0,  ...,    0,    0,    0],\n",
      "        [   0,    3,    0,  ...,    0,    0,    0],\n",
      "        [   0, 1934,    0,  ...,    0,    0,    0]]), tensor([150, 291, 248, 147, 215, 181, 172, 199]))\n",
      "torch.Size([8]) torch.Size([882, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([882, 8]) torch.Size([8])\n",
      "(tensor([1, 0, 0, 1, 0, 1, 0, 0]), tensor([[   2,   46,   12,  ...,   13,   14,   13],\n",
      "        [  74,   31, 2607,  ...,  428,   60,   94],\n",
      "        [   6,   65,    7,  ...,    8, 1856,    9],\n",
      "        ...,\n",
      "        [   0,  183,    0,  ...,    0,    0,    0],\n",
      "        [   0,   15,    0,  ...,    0,    0,    0],\n",
      "        [   0,    3,    0,  ...,    0,    0,    0]]), tensor([138, 882, 166,  86, 677, 159, 133, 576]))\n",
      "torch.Size([4]) torch.Size([172, 4]) torch.Size([4])\n",
      "torch.Size([4]) torch.Size([172, 4]) torch.Size([4])\n",
      "(tensor([1, 0, 0, 0]), tensor([[   13,     2,    13,    14],\n",
      "        [  228,   660,    45,   284],\n",
      "        [   14,  3403,  5538,     9],\n",
      "        [   23,     6,  1578,    27],\n",
      "        [   37, 22591,     9,    54],\n",
      "        [    2,    71,    16,    13],\n",
      "        [ 2942,   203,   129,   489],\n",
      "        [ 2070,     8,     4,     8],\n",
      "        [   23,    78,    13,    73],\n",
      "        [ 1340,     6,   267,     3],\n",
      "        [    4,    71,     2,    13],\n",
      "        [    6,    75,   218,  1204],\n",
      "        [   46,   172,    53,    14],\n",
      "        [   17,     8,  1654,    28],\n",
      "        [   29,    89,  6294,   271],\n",
      "        [    5,    20,     4,     6],\n",
      "        [ 2387,     2,    22,   491],\n",
      "        [  831,    74,  1643,     2],\n",
      "        [   12,     3,     9,    21],\n",
      "        [    2,     3,    27,    13],\n",
      "        [  339,     3,    25,   256],\n",
      "        [    3,    11,   246,   545],\n",
      "        [   11,     9,    24,  8704],\n",
      "        [   10,    16,  1417,     6],\n",
      "        [ 1038,   147,    11,   679],\n",
      "        [    8,    13,     8,   138],\n",
      "        [   73,    67,     5,     2],\n",
      "        [   29,    33,   655,    21],\n",
      "        [   70,   233,     3,    19],\n",
      "        [   54,    64,     2,     5],\n",
      "        [    5,  3188,    21,   562],\n",
      "        [   86,   332,    10,     3],\n",
      "        [  397,    12,     5, 12715],\n",
      "        [14456,    23,   121,    54],\n",
      "        [   10,   384,   438,    13],\n",
      "        [    4,     3,    79,   167],\n",
      "        [   22,    60,    13,     9],\n",
      "        [   96,   613,    17,    27],\n",
      "        [ 5493,     3,   991,    45],\n",
      "        [    2,  7667,     3,   462],\n",
      "        [  375,     4,    13,    39],\n",
      "        [    7,     2,    94,    77],\n",
      "        [    2,    74,     9,  1237],\n",
      "        [  705,    47,    27,    37],\n",
      "        [   10,   477,   171,     2],\n",
      "        [  104,     8, 56768,   337],\n",
      "        [    6,  2072,  2496,   294],\n",
      "        [   37,    28,     4,     2],\n",
      "        [  148,    19,    47, 14274],\n",
      "        [    5,    65,   377,    17],\n",
      "        [  196,  1736,     3,    71],\n",
      "        [  610,   289,   615,   660],\n",
      "        [    3,     3,    19,   447],\n",
      "        [   11,     3, 27585,     2],\n",
      "        [   91,     3,  7985,  4368],\n",
      "        [  105,    46,     4,    25],\n",
      "        [   78,    77,    56,   646],\n",
      "        [  110,   104,    57,    24],\n",
      "        [   96,   114,     4,   568],\n",
      "        [ 2005,   190,    56,     8],\n",
      "        [ 5043,    47,   385, 11298],\n",
      "        [   17,  1438,     3,   616],\n",
      "        [    4,    12,    51,     2],\n",
      "        [    6,    46,    26,    88],\n",
      "        [   96,    15,   188,  1609],\n",
      "        [   28,    75,     8,    80],\n",
      "        [  118,   172,   113,    30],\n",
      "        [  363,     8,     5,    17],\n",
      "        [   41,    89,    21,   660],\n",
      "        [ 1397,    20,    15,   595],\n",
      "        [    8,     2,   192,     2],\n",
      "        [ 4115,    74,  2496,   426],\n",
      "        [    6,     4,     4,   498],\n",
      "        [ 4726,    69,    22,    28],\n",
      "        [   12,   166,    10,     2],\n",
      "        [    5,    78,   170,   271],\n",
      "        [ 1166,   240,   159,    17],\n",
      "        [  103,    15,     4,    71],\n",
      "        [    4,     2,    73,    88],\n",
      "        [   39,  1495,   829,    54],\n",
      "        [14975,   705,  1419,    13],\n",
      "        [    3,   167,     9,   125],\n",
      "        [  285,     9,    16,    45],\n",
      "        [   26,    27,     9,   462],\n",
      "        [  703,    71,  2710,     2],\n",
      "        [   39,   127,   203,   212],\n",
      "        [   71,    54, 17033,    40],\n",
      "        [  454,    39,     9,  1572],\n",
      "        [   49,    77,     3, 13539],\n",
      "        [    2,   402,  3442,     3],\n",
      "        [ 2009,     4,     0,    14],\n",
      "        [    6,    48,     0,   212],\n",
      "        [   72,    47,     0,    10],\n",
      "        [  739,    15,     0,    71],\n",
      "        [   37,    11,     0,    86],\n",
      "        [   38,   149,     0,     3],\n",
      "        [    3,    33,     0,    13],\n",
      "        [   11,    85,     0,    33],\n",
      "        [    9,  6819,     0,     2],\n",
      "        [   16,     3,     0, 13872],\n",
      "        [    5,     3,     0,   736],\n",
      "        [  398,     3,     0,    43],\n",
      "        [    6,    69,     0,    14],\n",
      "        [  503,    67,     0,    21],\n",
      "        [ 7213,    33,     0,     3],\n",
      "        [  297,    85,     0,  1023],\n",
      "        [   74,     5,     0,     2],\n",
      "        [    3,  9339,     0,   458],\n",
      "        [    3,     4,     0,   498],\n",
      "        [    3,    29,     0,    10],\n",
      "        [  198,     5,     0,    29],\n",
      "        [  133,   851,     0,    57],\n",
      "        [19273,     3,     0,     4],\n",
      "        [   22,    18,     0,    22],\n",
      "        [   11,    13,     0,    99],\n",
      "        [    9,    33,     0,   577],\n",
      "        [   16,    47,     0,    19],\n",
      "        [   86,   295,     0,    99],\n",
      "        [    8,    38,     0,    68],\n",
      "        [   73,     7,     0,     3],\n",
      "        [   96,     2,     0,   294],\n",
      "        [ 2788,   662,     0,     2],\n",
      "        [    6,    66,     0,    98],\n",
      "        [ 3334,     8,     0,    68],\n",
      "        [ 2009,    14,     0,    76],\n",
      "        [    7,   231,     0,    73],\n",
      "        [  504,   140,     0,     2],\n",
      "        [   58,     2,     0,  4368],\n",
      "        [   34,   511,     0,    39],\n",
      "        [    3,  1297,     0,  4726],\n",
      "        [    0,     3,     0,     5],\n",
      "        [    0,     3,     0,   441],\n",
      "        [    0,     3,     0,     7],\n",
      "        [    0,    13,     0,  4712],\n",
      "        [    0,     9,     0,     3],\n",
      "        [    0,   251,     0,   447],\n",
      "        [    0,    33,     0,   929],\n",
      "        [    0,     8,     0,    10],\n",
      "        [    0,   141,     0,  1951],\n",
      "        [    0,    15,     0,   990],\n",
      "        [    0,    14,     0,    61],\n",
      "        [    0,    17,     0,    30],\n",
      "        [    0,    40,     0,    10],\n",
      "        [    0,   237,     0,    12],\n",
      "        [    0,     2,     0,     2],\n",
      "        [    0,   257,     0,   984],\n",
      "        [    0,     4,     0,     6],\n",
      "        [    0,     6,     0,    10],\n",
      "        [    0,    13,     0,   407],\n",
      "        [    0,    47,     0,   616],\n",
      "        [    0,   489,     0,    15],\n",
      "        [    0,     8,     0,   159],\n",
      "        [    0,  3072,     0,   137],\n",
      "        [    0,   405,     0,    20],\n",
      "        [    0,    29,     0,     2],\n",
      "        [    0,     8,     0, 10416],\n",
      "        [    0,   391,     0,     3],\n",
      "        [    0,    20,     0,   595],\n",
      "        [    0,    14,     0,     2],\n",
      "        [    0,    35,     0,   421],\n",
      "        [    0,     3,     0,    42],\n",
      "        [    0,     0,     0,  1075],\n",
      "        [    0,     0,     0,    14],\n",
      "        [    0,     0,     0,    23],\n",
      "        [    0,     0,     0,    67],\n",
      "        [    0,     0,     0,  2110],\n",
      "        [    0,     0,     0,   847],\n",
      "        [    0,     0,     0,    15],\n",
      "        [    0,     0,     0,   473],\n",
      "        [    0,     0,     0,   222],\n",
      "        [    0,     0,     0,  1991],\n",
      "        [    0,     0,     0,     3]]), tensor([130, 161,  90, 172]))\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i[0].shape, i[1].shape, i[2].shape)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate the performance of model, the frequently adopted solutions are cross validation and the usage of validation set. Since the size of training samples is small, cross validation would be a more appropriate strategy.\n",
    "\n",
    "1. RNN-based model\n",
    "2. Naive-bayes model\n",
    "3. Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline -TFiDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I watched this film because I'm a big fan of R...\n",
       "1        It does not seem that this movie managed to pl...\n",
       "2              Enough is not a bad movie , just mediocre .\n",
       "3        my friend and i rented this one a few nights a...\n",
       "4        Just about everything in this movie is wrong, ...\n",
       "                               ...                        \n",
       "29336    It 's one of the most honest films ever made a...\n",
       "29337    An absorbing and unsettling psychological drama .\n",
       "29338    Soylent Green IS...a really good movie, actual...\n",
       "29339    There just isn't enough here. There a few funn...\n",
       "29340    This show was absolutely terrible. For one Geo...\n",
       "Name: review, Length: 29341, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words='english', token_pattern='(?u)\\\\b[A-Za-z]+\\\\b')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "vectorizer.fit(drop_df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " '_check_n_features',\n",
       " '_check_params',\n",
       " '_check_stop_words_consistency',\n",
       " '_check_vocabulary',\n",
       " '_count_vocab',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_limit_features',\n",
       " '_more_tags',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_sort_features',\n",
       " '_stop_words_id',\n",
       " '_tfidf',\n",
       " '_validate_data',\n",
       " '_validate_params',\n",
       " '_validate_vocabulary',\n",
       " '_warn_for_unused_params',\n",
       " '_white_spaces',\n",
       " '_word_ngrams',\n",
       " 'analyzer',\n",
       " 'binary',\n",
       " 'build_analyzer',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'decode',\n",
       " 'decode_error',\n",
       " 'dtype',\n",
       " 'encoding',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'fixed_vocabulary_',\n",
       " 'get_feature_names',\n",
       " 'get_params',\n",
       " 'get_stop_words',\n",
       " 'idf_',\n",
       " 'input',\n",
       " 'inverse_transform',\n",
       " 'lowercase',\n",
       " 'max_df',\n",
       " 'max_features',\n",
       " 'min_df',\n",
       " 'ngram_range',\n",
       " 'norm',\n",
       " 'preprocessor',\n",
       " 'set_params',\n",
       " 'smooth_idf',\n",
       " 'stop_words',\n",
       " 'stop_words_',\n",
       " 'strip_accents',\n",
       " 'sublinear_tf',\n",
       " 'token_pattern',\n",
       " 'tokenizer',\n",
       " 'transform',\n",
       " 'use_idf',\n",
       " 'vocabulary',\n",
       " 'vocabulary_']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words='english', token_pattern='(?u)\\\\b[A-Za-z]+\\\\b')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "cv.fit(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212617"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = drop_df['clean_text'], drop_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    it does not seem that this movie managed to pl...\n",
       "1    a wonderful little production the filming tech...\n",
       "2            enough is not a bad movie  just mediocre \n",
       "3    my friend and i rented this one a few nights a...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[[1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, vectorizer, X, y):\n",
    "    prf = {\n",
    "        'accuracy': list(),\n",
    "        'precision': list(),\n",
    "        'recall': list(),\n",
    "        'f1-score': list()\n",
    "    }\n",
    "\n",
    "    for idx, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Cross validation {idx}-fold\")\n",
    "        X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "        X_valid, y_valid = X.iloc[valid_index], y.iloc[valid_index]\n",
    "\n",
    "        clf = model.fit(vectorizer.transform(X_train), y_train)\n",
    "        X_valid_transform = vectorizer.transform(X_valid)\n",
    "        y_preds = clf.predict(X_valid_transform)\n",
    "        results = precision_recall_fscore_support(y_valid, y_preds, average='binary')\n",
    "        print(results)\n",
    "        prf['accuracy'] += [clf.score(X_valid_transform, y_valid)]\n",
    "        prf['precision'] += [results[0]]\n",
    "        prf['recall'] += [results[1]]\n",
    "        prf['f1-score'] += [results[2]]\n",
    "    return prf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5329  5334  5337 ... 53957 53958 53959] [   0    1    2 ... 5452 5453 5458]\n",
      "Cross validation 0-fold\n",
      "(0.8533287101248266, 0.89981718464351, 0.8759565759031855, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [ 5329  5334  5337 ... 10858 10859 10860]\n",
      "Cross validation 1-fold\n",
      "(0.8612273361227336, 0.903107861060329, 0.8816705336426913, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [10728 10729 10730 ... 16310 16311 16312]\n",
      "Cross validation 2-fold\n",
      "(0.8589518114667605, 0.8928702010968922, 0.8755826461097168, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [16059 16060 16063 ... 21797 21800 21802]\n",
      "Cross validation 3-fold\n",
      "(0.8712800286841161, 0.8884826325411335, 0.8797972483707459, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [21366 21367 21372 ... 27154 27158 27160]\n",
      "Cross validation 4-fold\n",
      "(0.8610229276895943, 0.8925045703839123, 0.8764811490125674, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [26763 26770 26771 ... 32541 32546 32547]\n",
      "Cross validation 5-fold\n",
      "(0.8765652951699463, 0.8957952468007313, 0.8860759493670887, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [32234 32235 32237 ... 37804 37805 37806]\n",
      "Cross validation 6-fold\n",
      "(0.8833333333333333, 0.910453216374269, 0.8966882649388048, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [37730 37732 37733 ... 43330 43331 43332]\n",
      "Cross validation 7-fold\n",
      "(0.8757062146892656, 0.9064327485380117, 0.8908045977011494, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [42977 42979 42980 ... 48644 48645 48646]\n",
      "Cross validation 8-fold\n",
      "(0.887584489505514, 0.9119152046783626, 0.8995853614566433, None)\n",
      "[    0     1     2 ... 48644 48645 48646] [48489 48490 48491 ... 53957 53958 53959]\n",
      "Cross validation 9-fold\n",
      "(0.8941645523740486, 0.9016812865497076, 0.8979071883530482, None)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "prf = train(model, vectorizer, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8825796886582655\n",
      "precision 0.8723164699160139\n",
      "recall 0.900306015266686\n",
      "f1-score 0.8860549514855641\n"
     ]
    }
   ],
   "source": [
    "for k, v in prf.items():\n",
    "    print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(vectorizer.transform(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(vectorizer.transform(test_df['clean_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9273350630096368"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(vectorizer.transform(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = test_df\n",
    "submit_df['sentiment'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22622</td>\n",
       "      <td>Robert Lansing plays a scientist experimenting...</td>\n",
       "      <td>robert lansing plays a scientist experimenting...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10162</td>\n",
       "      <td>Well I've enjoy this movie, even though someti...</td>\n",
       "      <td>well i have enjoy this movie even though somet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17468</td>\n",
       "      <td>First things first - though I believe Joel Sch...</td>\n",
       "      <td>first things first  though i believe joel schu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42579</td>\n",
       "      <td>I watched this movie on the grounds that Amber...</td>\n",
       "      <td>i watched this movie on the grounds that amber...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701</td>\n",
       "      <td>A certain sexiness underlines even the dullest...</td>\n",
       "      <td>a certain sexiness underlines even the dullest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                             review  \\\n",
       "0  22622  Robert Lansing plays a scientist experimenting...   \n",
       "1  10162  Well I've enjoy this movie, even though someti...   \n",
       "2  17468  First things first - though I believe Joel Sch...   \n",
       "3  42579  I watched this movie on the grounds that Amber...   \n",
       "4    701  A certain sexiness underlines even the dullest...   \n",
       "\n",
       "                                          clean_text  sentiment  \n",
       "0  robert lansing plays a scientist experimenting...          0  \n",
       "1  well i have enjoy this movie even though somet...          1  \n",
       "2  first things first  though i believe joel schu...          0  \n",
       "3  i watched this movie on the grounds that amber...          0  \n",
       "4  a certain sexiness underlines even the dullest...          1  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = submit_df.drop(columns=['review', 'clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf + RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5329  5334  5337 ... 53957 53958 53959] [   0    1    2 ... 5452 5453 5458]\n",
      "Cross validation 0-fold\n",
      "(0.8253521126760563, 0.8570383912248629, 0.8408968609865471, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [ 5329  5334  5337 ... 10858 10859 10860]\n",
      "Cross validation 1-fold\n",
      "(0.8356115107913669, 0.8493601462522852, 0.842429737080689, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [10728 10729 10730 ... 16310 16311 16312]\n",
      "Cross validation 2-fold\n",
      "(0.8070475538829969, 0.8625228519195612, 0.8338635560268647, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [16059 16060 16063 ... 21797 21800 21802]\n",
      "Cross validation 3-fold\n",
      "(0.8240312833274085, 0.8475319926873858, 0.8356164383561645, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [21366 21367 21372 ... 27154 27158 27160]\n",
      "Cross validation 4-fold\n",
      "(0.8162199791159067, 0.8574040219378428, 0.8363052781740372, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [26763 26770 26771 ... 32541 32546 32547]\n",
      "Cross validation 5-fold\n",
      "(0.8534798534798534, 0.8519195612431444, 0.8526989935956084, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [32234 32235 32237 ... 37804 37805 37806]\n",
      "Cross validation 6-fold\n",
      "(0.8627812615271118, 0.8548976608187134, 0.8588213695612262, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [37730 37732 37733 ... 43330 43331 43332]\n",
      "Cross validation 7-fold\n",
      "(0.8624123939505718, 0.85453216374269, 0.8584541949697081, None)\n",
      "[    0     1     2 ... 53957 53958 53959] [42977 42979 42980 ... 48644 48645 48646]\n",
      "Cross validation 8-fold\n",
      "(0.8662490788504053, 0.8592836257309941, 0.8627522935779817, None)\n",
      "[    0     1     2 ... 48644 48645 48646] [48489 48490 48491 ... 53957 53958 53959]\n",
      "Cross validation 9-fold\n",
      "(0.8588278658311832, 0.8516081871345029, 0.855202789502661, None)\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "prf = train(model, vectorizer, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8441808747220163\n",
      "precision 0.8412012893432861\n",
      "recall 0.8546098602691983\n",
      "f1-score 0.8477041511831489\n"
     ]
    }
   ],
   "source": [
    "for k, v in prf.items():\n",
    "    print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 0-fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hlu/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8485639686684073, 0.8742434431741762, 0.8612123219609142, None)\n",
      "Cross validation 1-fold\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fp/kr0zqp696h3bm6dmv7lmb_jc0000gn/T/ipykernel_8573/2231321337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fp/kr0zqp696h3bm6dmv7lmb_jc0000gn/T/ipykernel_8573/1541409531.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, vectorizer, X)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mX_valid_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1870\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1871\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[0;34m(self, tokens, stop_words)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# handle stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# handle token n-grams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# handle stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# handle token n-grams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "prf = train(model, vectorizer, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = list(zip(train_df['review'], train_df['sentiment']))\n",
    "num_class = len(set([label for (text, label) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, F.one_hot(label, num_classes=num_class).type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_count = 0\n",
    "    all_preds, all_labels = list(), list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            predicted_label = predicted_label.argmax(1)\n",
    "            all_preds += [predicted_label.detach().numpy()]\n",
    "            all_labels += [label.detach().numpy()]\n",
    "            total_count += label.size(0)\n",
    "    print(all_preds, all_labels, total_count)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    prf = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    print(prf)\n",
    "    return (all_preds == all_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassificationModel(\n",
      "  (embedding): EmbeddingBag(102971, 64, mode=mean)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Cross validation 0-fold\n",
      "[array([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1]), array([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0])] [array([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0])] 100\n",
      "(0.6933333333333334, 1.0, 0.8188976377952756, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.45s | valid accuracy    0.770 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]), array([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0])] [array([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]), array([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1])] 100\n",
      "(1.0, 0.5961538461538461, 0.7469879518072289, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.49s | valid accuracy    0.790 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0]), array([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])] [array([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "       0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1]), array([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1])] 100\n",
      "(0.972972972972973, 0.6923076923076923, 0.8089887640449438, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.46s | valid accuracy    0.830 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 1-fold\n",
      "[array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0]), array([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1])] [array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0]), array([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1])] 100\n",
      "(0.9423076923076923, 1.0, 0.9702970297029703, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.970 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]), array([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1])] [array([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]), array([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0])] 100\n",
      "(0.875, 1.0, 0.9333333333333333, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.43s | valid accuracy    0.930 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0]), array([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "       0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0])] [array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0]), array([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "       0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0])] 100\n",
      "(0.9245283018867925, 1.0, 0.9607843137254902, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.47s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 2-fold\n",
      "[array([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1]), array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1])] [array([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1]), array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1])] 100\n",
      "(0.9245283018867925, 0.98, 0.9514563106796116, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]), array([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "       0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1])] [array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]), array([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "       0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1])] 100\n",
      "(0.9245283018867925, 0.98, 0.9514563106796116, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1]), array([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "       1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1])] [array([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1]), array([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1])] 100\n",
      "(0.9245283018867925, 0.98, 0.9514563106796116, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.46s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 3-fold\n",
      "[array([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0]), array([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0])] [array([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0]), array([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0])] 100\n",
      "(0.9433962264150944, 0.9803921568627451, 0.9615384615384616, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]), array([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "       1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1])] [array([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]), array([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "       1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1])] 100\n",
      "(0.9433962264150944, 0.9803921568627451, 0.9615384615384616, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.44s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]), array([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0])] [array([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]), array([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0])] 100\n",
      "(0.9433962264150944, 0.9803921568627451, 0.9615384615384616, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.45s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 4-fold\n",
      "[array([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0]), array([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1])] [array([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "       0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0]), array([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1])] 100\n",
      "(0.9534883720930233, 0.9318181818181818, 0.942528735632184, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1]), array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0])] [array([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1]), array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0])] 100\n",
      "(0.9534883720930233, 0.9318181818181818, 0.942528735632184, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]), array([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0])] [array([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0])] 100\n",
      "(0.9534883720930233, 0.9318181818181818, 0.942528735632184, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.45s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 5-fold\n",
      "[array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]), array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0])] [array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]), array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0])] 100\n",
      "(0.9285714285714286, 0.9512195121951219, 0.9397590361445782, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1]), array([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0])] [array([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]), array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0])] 100\n",
      "(0.9285714285714286, 0.9512195121951219, 0.9397590361445782, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.45s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]), array([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0])] [array([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]), array([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0])] 100\n",
      "(0.9285714285714286, 0.9512195121951219, 0.9397590361445782, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 6-fold\n",
      "[array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0]), array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1])] [array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0]), array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "       1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1])] 100\n",
      "(0.9824561403508771, 0.9655172413793104, 0.9739130434782608, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.45s | valid accuracy    0.970 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]), array([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "       0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0])] [array([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]), array([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0])] 100\n",
      "(0.9824561403508771, 0.9655172413793104, 0.9739130434782608, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.43s | valid accuracy    0.970 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1]), array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "       0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0])] [array([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1]), array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "       0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0])] 100\n",
      "(0.9824561403508771, 0.9655172413793104, 0.9739130434782608, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.45s | valid accuracy    0.970 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 7-fold\n",
      "[array([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1]), array([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1])] [array([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1]), array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1])] 100\n",
      "(1.0, 0.9107142857142857, 0.9532710280373832, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.43s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1])] [array([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1])] 100\n",
      "(1.0, 0.9107142857142857, 0.9532710280373832, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.43s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]), array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0])] [array([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]), array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0])] 100\n",
      "(1.0, 0.9107142857142857, 0.9532710280373832, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.42s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 8-fold\n",
      "[array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1]), array([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "       1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0])] [array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1]), array([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0])] 100\n",
      "(0.9591836734693877, 0.9591836734693877, 0.9591836734693877, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.45s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]), array([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1])] [array([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]), array([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1])] 100\n",
      "(0.94, 0.9591836734693877, 0.9494949494949495, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.46s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]), array([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1])] [array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]), array([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1])] 100\n",
      "(0.94, 0.9591836734693877, 0.9494949494949495, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 9-fold\n",
      "[array([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]), array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1])] [array([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0]), array([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1])] 100\n",
      "(0.9607843137254902, 0.9423076923076923, 0.9514563106796117, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.45s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]), array([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1])] [array([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0]), array([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1])] 100\n",
      "(0.9607843137254902, 0.9423076923076923, 0.9514563106796117, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.45s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]), array([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0])] [array([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]), array([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0])] 100\n",
      "(0.9607843137254902, 0.9423076923076923, 0.9514563106796117, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.49s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[0.83, 0.97, 0.95, 0.96, 0.95, 0.95, 0.97, 0.95, 0.96, 0.95]\n",
      "0.9439999999999997\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "# Hyperparameters\n",
    "EPOCHS = 1 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# train_iter = list(zip(train_df.iloc[:1000]['review'], train_df.iloc[:1000]['sentiment']))\n",
    "\n",
    "X, y = train_df.iloc[:1000]['review'], train_df['sentiment'].iloc[:1000].to_numpy()\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# train_dataset = list(train_iter)\n",
    "# test_dataset = list(test_iter)\n",
    "# num_train = int(len(train_dataset) * 0.95)\n",
    "# split_train_, split_valid_ = \\\n",
    "#     random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "# train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "#                               shuffle=True, collate_fn=collate_batch)\n",
    "# valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "#                               shuffle=True, collate_fn=collate_batch)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "#                              shuffle=True, collate_fn=collate_batch)\n",
    "print(model)\n",
    "total_acc = []\n",
    "for idx, (train_index, valid_index) in enumerate(kf.split(X)):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(f\"Cross validation {idx}-fold\")\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "    train_iter = list(zip(X_train, y_train))\n",
    "    valid_iter = list(zip(X_valid, y_valid))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "    valid_dataloader = DataLoader(valid_iter, batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True, collate_fn=collate_batch)\n",
    "    cross_acc = None\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_dataloader)\n",
    "        acc = evaluate(valid_dataloader)\n",
    "        if cross_acc is not None and cross_acc > acc:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            cross_acc = acc\n",
    "        print('-' * 59)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "              'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                               time.time() - epoch_start_time,\n",
    "                                                acc))\n",
    "        print('-' * 59)\n",
    "    total_acc += [cross_acc]\n",
    "print(total_acc)\n",
    "print(np.mean(total_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RNN-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    # target is hidden_size\n",
    "    def __init__(self, hidden_size, method='concat'):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        if method not in ('dot', 'general', 'concat'):\n",
    "            raise NotImplemented\n",
    "        if method == 'general':\n",
    "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        elif method == 'concat':\n",
    "            self.attn = nn.Linear(2 * hidden_size, hidden_size)\n",
    "            self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        if hasattr(self, 'attn'):\n",
    "            self.attn.weight.data.uniform_(-initrange, initrange)\n",
    "            self.attn.bias.data.zero_()\n",
    "        if hasattr(self, 'v'):\n",
    "            self.v.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.matmul(hidden, encoder_output)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        attn = self.attn(encoder_output)\n",
    "        return torch.matmul(hidden, attn)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        hidden_reshape = torch.unsqueeze(hidden, dim=0).repeat(encoder_output.size(0), 1, 1)\n",
    "        attn = self.attn(torch.cat([hidden_reshape, encoder_output], dim=-1)).tanh()\n",
    "        return self.v(attn).squeeze(dim=-1)\n",
    "\n",
    "    def forward(self, hidden, encoder_output):\n",
    "        # output = [lengths x batch_size x hidden_size]\n",
    "        # hidden = [batch_size x hidden_size]\n",
    "        attn_scores = None\n",
    "        if self.method == 'dot':\n",
    "            attn_scores = self.dot_score(hidden, encoder_output)\n",
    "        elif self.method == 'general':\n",
    "            attn_scores = self.general_score(hidden, encoder_output)\n",
    "        elif self.method == 'concat':\n",
    "            attn_scores = self.concat_score(hidden, encoder_output)\n",
    "\n",
    "        # [lengths x batch_size] -> [batch_size x lengths]\n",
    "        attn_scores = attn_scores.t()\n",
    "        # return [batch_size x 1 x lengths]\n",
    "        return F.softmax(attn_scores, dim=-1).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 5, 3, 2, 1])\n",
      "tensor([5, 3, 7, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([5,3,7,2,1])\n",
    "sorted_t, idx = t.sort(descending=True)\n",
    "print(sorted_t)\n",
    "print(torch.gather(t, 0, torch.arange(0, idx.shape[0], dtype=torch.int64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "def sort_sequence(inputs, lengths):\n",
    "    sorted_lengths, sorted_idx = lengths.sort(descending=True)\n",
    "    return inputs[sorted_idx], sorted_lengths, sorted_idx\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, n_layers, dropout, num_classes, attention_mode, padding_idx=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, sparse=True, padding_idx=1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, n_layers, dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        self.attn = Attention(2 * hidden_size, attention_mode)\n",
    "        self.fc = nn.Linear(2 * hidden_size, num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        for param in self.lstm.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.uniform_(param.data, -initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, text_lengths, hidden=None):\n",
    "        sorted_lengths, sorted_idx = text_lengths.sort(descending=True)\n",
    "        sorted_text = torch.index_select(text, -1, sorted_idx)\n",
    "        emb = self.embedding(sorted_text)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, sorted_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        hidden_state, _ = hidden\n",
    "        hidden_state = hidden_state[-2:,:,:].view(1, -1, 2 * hidden_size).squeeze(0)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        attn_weights = self.attn(hidden_state, outputs)\n",
    "        # attn_weights = [batch_size x 1 x lengths]\n",
    "        context = torch.bmm(attn_weights, outputs.transpose(0, 1)).squeeze(1)\n",
    "        output = self.fc(context)\n",
    "        output = torch.index_select(output, 0, torch.arange(0, sorted_idx.shape[0], dtype=torch.int64))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50\n",
    "hidden_size = 256\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "num_classes = 2\n",
    "attention_mode = 'concat'\n",
    "model = LSTMModel(vocab_size, embed_size, hidden_size, n_layers, \n",
    "                  dropout, num_classes, attention_mode).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label, _ = model(text, offsets)\n",
    "        loss = criterion(predited_label, F.one_hot(label, num_classes=num_class).type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_count = 0\n",
    "    all_preds, all_labels = list(), list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label, _ = model(text, offsets)\n",
    "            predicted_label = predicted_label.argmax(1)\n",
    "            all_preds += [predicted_label.detach().numpy()]\n",
    "            all_labels += [label.detach().numpy()]\n",
    "            total_count += label.size(0)\n",
    "    print(all_preds, all_labels, total_count)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    prf = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    print(prf)\n",
    "    return (all_preds == all_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embedding): Embedding(102971, 50, padding_idx=1, sparse=True)\n",
      "  (lstm): LSTM(50, 256, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (attn): Attention(\n",
      "    (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "Cross validation 0-fold\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([4, 2])\n",
      "[array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0])] [array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 0, 0, 0, 1]), array([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "       1, 1, 0, 0, 1, 0, 1, 1, 1, 1]), array([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 0, 1, 0]), array([1, 0, 0, 1])] 100\n",
      "(0.5, 0.038461538461538464, 0.07142857142857144, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 1770.09s | valid accuracy    0.480 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 1-fold\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([4, 2])\n",
      "[array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0])] [array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       1, 1, 1, 0, 1, 0, 0, 1, 0, 1]), array([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 1, 0, 1, 0, 0, 0, 1, 1]), array([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1]), array([0, 1, 1, 1])] 100\n",
      "(0.5, 0.04081632653061224, 0.07547169811320753, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 1824.12s | valid accuracy    0.510 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 2-fold\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "# Hyperparameters\n",
    "EPOCHS = 1 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 32 # batch size for training\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "X, y = train_df.iloc[:1000]['review'], train_df['sentiment'].iloc[:1000].to_numpy()\n",
    "print(model)\n",
    "total_acc = []\n",
    "for idx, (train_index, valid_index) in enumerate(kf.split(X)):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(f\"Cross validation {idx}-fold\")\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "    train_iter = list(zip(X_train, y_train))\n",
    "    valid_iter = list(zip(X_valid, y_valid))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "    valid_dataloader = DataLoader(valid_iter, batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True, collate_fn=collate_batch)\n",
    "    cross_acc = None\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_dataloader)\n",
    "        acc = evaluate(valid_dataloader)\n",
    "        if cross_acc is not None and cross_acc > acc:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            cross_acc = acc\n",
    "        print('-' * 59)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "              'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                               time.time() - epoch_start_time,\n",
    "                                                acc))\n",
    "        print('-' * 59)\n",
    "    total_acc += [cross_acc]\n",
    "print(total_acc)\n",
    "print(np.mean(total_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4545a62de3741b5917f48d7da05bb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  I am a good person, and how about you?\n",
      "Tokenized:  ['i', 'am', 'a', 'good', 'person', ',', 'and', 'how', 'about', 'you', '?']\n",
      "Token IDs:  [101, 1045, 2572, 1037, 2204, 2711, 1010, 1998, 2129, 2055, 2017, 1029, 100]\n"
     ]
    }
   ],
   "source": [
    "text = \"I am a good person, and how about you?\"\n",
    "print(' Original: ', text)\n",
    "print('Tokenized: ', tokenizer.tokenize(text))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokenizer.tokenize(text)+ [\"SEP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2572, 1037, 2204, 2711, 1010, 1998, 2129, 2055, 2017, 1029, 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/hlu/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1938: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2572, 1037, 2204, 2711, 1010, 1998, 2129, 2055, 2017, 1029,\n",
       "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\n",
    "                text, add_special_tokens=True, max_length=64,\n",
    "                pad_to_max_length=True, return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bec40be47d84561a3bd779be34845ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a9022900a040238fe3486506b041ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "from transformers import XLNetModel, XLNetForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = XLNetModel.from_pretrained('xlnet-base-cased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetModel(\n",
       "  (word_embedding): Embedding(32000, 768)\n",
       "  (layer): ModuleList(\n",
       "    (0): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am a good boy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', 'i', '▁am', '▁a', '▁good', '▁boy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict = tokenizer.encode_plus(\n",
    "    \"<cls> asd11l2js I am a good boy\", add_special_tokens=True, max_length=50,\n",
    "    pad_to_max_length=True, return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "            5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "            5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    3,   34,\n",
       "           66, 1545,  368,  184, 1315,   23,   17,  150,  569,   24,  195, 2001,\n",
       "            4,    3]]), 'token_type_ids': tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 2]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list, attention_masks = list(), list()\n",
    "text_list.append(encoded_dict['input_ids'])\n",
    "attention_masks.append(encoded_dict['attention_mask'])\n",
    "text_list = torch.cat(text_list, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(text_list, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetModelOutput(last_hidden_state=tensor([[[ 1.4764,  2.5515, -0.8054,  ..., -0.7408,  0.5854, -2.2659],\n",
       "         [ 1.6141,  2.4739, -0.7761,  ..., -0.7215,  0.4234, -2.3880],\n",
       "         [ 1.5958,  2.4999, -0.7793,  ..., -0.6517,  0.2706, -2.4049],\n",
       "         ...,\n",
       "         [ 3.8463,  0.2613, -1.5299,  ..., -2.6709, -0.2188,  0.2802],\n",
       "         [ 5.7440, -0.2734, -3.4873,  ..., -3.3266,  0.4171,  0.4994],\n",
       "         [ 3.7563, -0.2951, -2.6874,  ..., -1.8446,  0.5208,  0.9403]]],\n",
       "       grad_fn=<PermuteBackward>), mems=(tensor([[[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252]],\n",
       "\n",
       "        [[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252]],\n",
       "\n",
       "        [[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0540,  0.0375,  0.0332,  ...,  0.0201, -0.0480,  0.0724]],\n",
       "\n",
       "        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n",
       "\n",
       "        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[ 0.9156,  1.0634, -0.2741,  ..., -0.1662, -0.1363, -0.8428]],\n",
       "\n",
       "        [[ 0.9461,  1.0958, -0.2464,  ..., -0.1756, -0.1703, -0.8440]],\n",
       "\n",
       "        [[ 0.9758,  1.1076, -0.2352,  ..., -0.1836, -0.1847, -0.8192]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.8328,  0.5979,  0.6304,  ..., -0.4727, -1.2793,  1.6664]],\n",
       "\n",
       "        [[ 1.0250, -0.4521, -1.5119,  ..., -0.0787,  0.4344,  0.7233]],\n",
       "\n",
       "        [[ 0.1938,  0.1017, -2.3535,  ..., -0.5091, -0.1052,  1.6627]]]), tensor([[[ 0.3933,  0.3043, -0.2023,  ...,  0.1985,  0.2346, -0.5402]],\n",
       "\n",
       "        [[ 0.4375,  0.3185, -0.1855,  ...,  0.1863,  0.1941, -0.5373]],\n",
       "\n",
       "        [[ 0.4548,  0.3255, -0.1658,  ...,  0.1743,  0.1989, -0.5231]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0949,  0.1630,  0.7266,  ...,  0.1444, -1.3562,  1.3974]],\n",
       "\n",
       "        [[ 0.3575, -0.6885, -1.4503,  ..., -0.9342,  0.5952,  0.3722]],\n",
       "\n",
       "        [[-0.0512, -0.6750, -2.2458,  ..., -0.7432,  0.3301,  1.2587]]]), tensor([[[ 0.2071,  0.9489, -0.6494,  ...,  0.9213, -0.2973, -0.9567]],\n",
       "\n",
       "        [[ 0.2374,  0.9585, -0.6248,  ...,  0.9006, -0.3489, -0.9238]],\n",
       "\n",
       "        [[ 0.2464,  0.9809, -0.6202,  ...,  0.8855, -0.3586, -0.8963]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7476,  0.6471,  0.5672,  ..., -0.3418, -1.2544,  0.5400]],\n",
       "\n",
       "        [[-0.1788,  0.2359, -1.5412,  ..., -0.8350,  0.2809, -0.4816]],\n",
       "\n",
       "        [[-0.2697, -0.3103, -2.6733,  ..., -0.4990, -0.2893,  0.5538]]]), tensor([[[ 1.4693,  0.0391, -0.7497,  ...,  1.5123, -0.5623, -0.5832]],\n",
       "\n",
       "        [[ 1.5013,  0.0470, -0.7449,  ...,  1.5048, -0.6189, -0.5611]],\n",
       "\n",
       "        [[ 1.5103,  0.0562, -0.7554,  ...,  1.4908, -0.6479, -0.5522]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5714, -0.2590,  0.1343,  ...,  0.4249, -0.8150,  0.3106]],\n",
       "\n",
       "        [[ 0.5622, -0.4188, -1.8813,  ..., -0.0362, -0.3020, -0.0747]],\n",
       "\n",
       "        [[ 0.7146, -0.7559, -2.6028,  ...,  0.1587, -0.5205,  0.1761]]]), tensor([[[ 1.3464,  0.4357, -0.9102,  ...,  0.5386, -1.3498, -1.5001]],\n",
       "\n",
       "        [[ 1.3908,  0.4392, -0.9228,  ...,  0.5474, -1.3786, -1.4790]],\n",
       "\n",
       "        [[ 1.4049,  0.4384, -0.9358,  ...,  0.5331, -1.3844, -1.4777]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.3468,  0.6591,  0.5007,  ..., -0.4910, -0.7752, -0.1183]],\n",
       "\n",
       "        [[ 0.3819,  0.3082, -1.4197,  ...,  0.0685, -0.3215, -0.7804]],\n",
       "\n",
       "        [[ 0.5596, -0.0037, -2.2351,  ...,  0.0812, -0.5723, -0.3554]]]), tensor([[[ 1.0619,  0.6954, -2.0038,  ..., -0.3962, -1.9340, -1.0415]],\n",
       "\n",
       "        [[ 1.1375,  0.7019, -1.9848,  ..., -0.3725, -2.0207, -0.9980]],\n",
       "\n",
       "        [[ 1.2100,  0.6881, -1.9637,  ..., -0.3556, -2.0325, -1.0247]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.4129,  0.9158,  0.6443,  ..., -0.9858, -1.9156,  0.5450]],\n",
       "\n",
       "        [[ 0.5878,  0.1117, -1.9039,  ..., -0.2968, -0.6944, -0.0989]],\n",
       "\n",
       "        [[ 0.0981, -0.2018, -2.8803,  ..., -0.3664, -0.9444,  0.1085]]]), tensor([[[ 0.6015,  1.1536, -1.6290,  ..., -0.6842, -1.4619, -0.5704]],\n",
       "\n",
       "        [[ 0.6530,  1.1733, -1.6237,  ..., -0.6925, -1.5167, -0.5520]],\n",
       "\n",
       "        [[ 0.6745,  1.1747, -1.6395,  ..., -0.6756, -1.5211, -0.5644]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.9452,  1.1977,  1.3949,  ..., -1.0470, -0.7192,  1.2919]],\n",
       "\n",
       "        [[ 0.7428,  1.1699, -1.1035,  ..., -1.0125, -0.3954,  0.8313]],\n",
       "\n",
       "        [[-0.2416,  1.0022, -1.9961,  ..., -0.9663, -0.4888,  0.9290]]]), tensor([[[ 0.8313,  1.3704, -1.9395,  ..., -0.0134, -1.8234, -0.5403]],\n",
       "\n",
       "        [[ 0.9062,  1.3974, -1.9473,  ..., -0.0270, -1.8912, -0.4990]],\n",
       "\n",
       "        [[ 0.9857,  1.4109, -1.9736,  ..., -0.0311, -1.9189, -0.5002]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5132,  1.5491,  0.5664,  ..., -0.8778, -1.2237,  0.7145]],\n",
       "\n",
       "        [[ 1.4038,  1.0113, -1.2193,  ..., -1.1647, -0.5987, -0.3652]],\n",
       "\n",
       "        [[-0.1021,  1.2665, -2.0035,  ..., -0.8056, -0.1584, -0.0238]]]), tensor([[[ 0.6594,  0.8182, -2.0167,  ..., -0.5913, -0.6711, -0.3155]],\n",
       "\n",
       "        [[ 0.7234,  0.7991, -1.9943,  ..., -0.6506, -0.7660, -0.2838]],\n",
       "\n",
       "        [[ 0.7489,  0.8035, -2.0283,  ..., -0.6751, -0.8198, -0.2729]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7829,  0.5609,  0.3888,  ..., -1.3396, -0.7099,  1.1600]],\n",
       "\n",
       "        [[ 0.9701,  0.0308, -1.4273,  ..., -1.8822,  0.3705, -0.0501]],\n",
       "\n",
       "        [[ 0.0121,  0.6457, -1.9682,  ..., -0.9920,  0.6946,  0.0159]]]), tensor([[[ 0.7801,  0.9696, -2.0823,  ..., -0.8309, -0.3148,  0.0946]],\n",
       "\n",
       "        [[ 0.8911,  0.9265, -2.0851,  ..., -0.8720, -0.3838,  0.0760]],\n",
       "\n",
       "        [[ 0.9231,  0.9462, -2.1496,  ..., -0.8830, -0.4642,  0.0740]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0418,  0.4412,  0.4693,  ..., -1.4395, -1.1413,  1.8067]],\n",
       "\n",
       "        [[ 1.3913,  0.0687, -1.5188,  ..., -2.0519, -0.1029,  1.2681]],\n",
       "\n",
       "        [[ 0.3388,  0.8382, -1.4421,  ..., -0.9915,  0.1114,  1.0360]]]), tensor([[[ 0.5958,  0.9143, -0.8302,  ..., -0.0795,  0.0356,  0.3544]],\n",
       "\n",
       "        [[ 0.6664,  0.8663, -0.8206,  ..., -0.0660, -0.0228,  0.3231]],\n",
       "\n",
       "        [[ 0.6616,  0.8829, -0.8320,  ..., -0.0452, -0.0994,  0.2967]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3205, -0.6818, -0.0454,  ..., -0.6727, -0.7787,  1.1092]],\n",
       "\n",
       "        [[ 1.3218, -0.7020, -1.0760,  ..., -0.9728, -0.2405,  0.8265]],\n",
       "\n",
       "        [[ 0.2434, -0.3605, -0.8424,  ..., -0.0908, -0.1475,  0.6281]]])), hidden_states=(tensor([[[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "         [ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "         [ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "         ...,\n",
       "         [ 0.0540,  0.0375,  0.0332,  ...,  0.0201, -0.0480,  0.0724],\n",
       "         [ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520],\n",
       "         [ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.9156,  1.0634, -0.2741,  ..., -0.1662, -0.1363, -0.8428],\n",
       "         [ 0.9461,  1.0958, -0.2464,  ..., -0.1756, -0.1703, -0.8440],\n",
       "         [ 0.9758,  1.1076, -0.2352,  ..., -0.1836, -0.1847, -0.8192],\n",
       "         ...,\n",
       "         [ 0.8328,  0.5979,  0.6304,  ..., -0.4727, -1.2793,  1.6664],\n",
       "         [ 1.0250, -0.4521, -1.5119,  ..., -0.0787,  0.4344,  0.7233],\n",
       "         [ 0.1938,  0.1017, -2.3535,  ..., -0.5091, -0.1052,  1.6627]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.3933,  0.3043, -0.2023,  ...,  0.1985,  0.2346, -0.5402],\n",
       "         [ 0.4375,  0.3185, -0.1855,  ...,  0.1863,  0.1941, -0.5373],\n",
       "         [ 0.4548,  0.3255, -0.1658,  ...,  0.1743,  0.1989, -0.5231],\n",
       "         ...,\n",
       "         [-0.0949,  0.1630,  0.7266,  ...,  0.1444, -1.3562,  1.3974],\n",
       "         [ 0.3575, -0.6885, -1.4503,  ..., -0.9342,  0.5952,  0.3722],\n",
       "         [-0.0512, -0.6750, -2.2458,  ..., -0.7432,  0.3301,  1.2587]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.2071,  0.9489, -0.6494,  ...,  0.9213, -0.2973, -0.9567],\n",
       "         [ 0.2374,  0.9585, -0.6248,  ...,  0.9006, -0.3489, -0.9238],\n",
       "         [ 0.2464,  0.9809, -0.6202,  ...,  0.8855, -0.3586, -0.8963],\n",
       "         ...,\n",
       "         [-0.7476,  0.6471,  0.5672,  ..., -0.3418, -1.2544,  0.5400],\n",
       "         [-0.1788,  0.2359, -1.5412,  ..., -0.8350,  0.2809, -0.4816],\n",
       "         [-0.2697, -0.3103, -2.6733,  ..., -0.4990, -0.2893,  0.5538]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 1.4693,  0.0391, -0.7497,  ...,  1.5123, -0.5623, -0.5832],\n",
       "         [ 1.5013,  0.0470, -0.7449,  ...,  1.5048, -0.6189, -0.5611],\n",
       "         [ 1.5103,  0.0562, -0.7554,  ...,  1.4908, -0.6479, -0.5522],\n",
       "         ...,\n",
       "         [-0.5714, -0.2590,  0.1343,  ...,  0.4249, -0.8150,  0.3106],\n",
       "         [ 0.5622, -0.4188, -1.8813,  ..., -0.0362, -0.3020, -0.0747],\n",
       "         [ 0.7146, -0.7559, -2.6028,  ...,  0.1587, -0.5205,  0.1761]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 1.3464,  0.4357, -0.9102,  ...,  0.5386, -1.3498, -1.5001],\n",
       "         [ 1.3908,  0.4392, -0.9228,  ...,  0.5474, -1.3786, -1.4790],\n",
       "         [ 1.4049,  0.4384, -0.9358,  ...,  0.5331, -1.3844, -1.4777],\n",
       "         ...,\n",
       "         [-1.3468,  0.6591,  0.5007,  ..., -0.4910, -0.7752, -0.1183],\n",
       "         [ 0.3819,  0.3082, -1.4197,  ...,  0.0685, -0.3215, -0.7804],\n",
       "         [ 0.5596, -0.0037, -2.2351,  ...,  0.0812, -0.5723, -0.3554]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 1.0619,  0.6954, -2.0038,  ..., -0.3962, -1.9340, -1.0415],\n",
       "         [ 1.1375,  0.7019, -1.9848,  ..., -0.3725, -2.0207, -0.9980],\n",
       "         [ 1.2100,  0.6881, -1.9637,  ..., -0.3556, -2.0325, -1.0247],\n",
       "         ...,\n",
       "         [-1.4129,  0.9158,  0.6443,  ..., -0.9858, -1.9156,  0.5450],\n",
       "         [ 0.5878,  0.1117, -1.9039,  ..., -0.2968, -0.6944, -0.0989],\n",
       "         [ 0.0981, -0.2018, -2.8803,  ..., -0.3664, -0.9444,  0.1085]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.6015,  1.1536, -1.6290,  ..., -0.6842, -1.4619, -0.5704],\n",
       "         [ 0.6530,  1.1733, -1.6237,  ..., -0.6925, -1.5167, -0.5520],\n",
       "         [ 0.6745,  1.1747, -1.6395,  ..., -0.6756, -1.5211, -0.5644],\n",
       "         ...,\n",
       "         [-0.9452,  1.1977,  1.3949,  ..., -1.0470, -0.7192,  1.2919],\n",
       "         [ 0.7428,  1.1699, -1.1035,  ..., -1.0125, -0.3954,  0.8313],\n",
       "         [-0.2416,  1.0022, -1.9961,  ..., -0.9663, -0.4888,  0.9290]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.8313,  1.3704, -1.9395,  ..., -0.0134, -1.8234, -0.5403],\n",
       "         [ 0.9062,  1.3974, -1.9473,  ..., -0.0270, -1.8912, -0.4990],\n",
       "         [ 0.9857,  1.4109, -1.9736,  ..., -0.0311, -1.9189, -0.5002],\n",
       "         ...,\n",
       "         [-0.5132,  1.5491,  0.5664,  ..., -0.8778, -1.2237,  0.7145],\n",
       "         [ 1.4038,  1.0113, -1.2193,  ..., -1.1647, -0.5987, -0.3652],\n",
       "         [-0.1021,  1.2665, -2.0035,  ..., -0.8056, -0.1584, -0.0238]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.6594,  0.8182, -2.0167,  ..., -0.5913, -0.6711, -0.3155],\n",
       "         [ 0.7234,  0.7991, -1.9943,  ..., -0.6506, -0.7660, -0.2838],\n",
       "         [ 0.7489,  0.8035, -2.0283,  ..., -0.6751, -0.8198, -0.2729],\n",
       "         ...,\n",
       "         [-0.7829,  0.5609,  0.3888,  ..., -1.3396, -0.7099,  1.1600],\n",
       "         [ 0.9701,  0.0308, -1.4273,  ..., -1.8822,  0.3705, -0.0501],\n",
       "         [ 0.0121,  0.6457, -1.9682,  ..., -0.9920,  0.6946,  0.0159]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.7801,  0.9696, -2.0823,  ..., -0.8309, -0.3148,  0.0946],\n",
       "         [ 0.8911,  0.9265, -2.0851,  ..., -0.8720, -0.3838,  0.0760],\n",
       "         [ 0.9231,  0.9462, -2.1496,  ..., -0.8830, -0.4642,  0.0740],\n",
       "         ...,\n",
       "         [-0.0418,  0.4412,  0.4693,  ..., -1.4395, -1.1413,  1.8067],\n",
       "         [ 1.3913,  0.0687, -1.5188,  ..., -2.0519, -0.1029,  1.2681],\n",
       "         [ 0.3388,  0.8382, -1.4421,  ..., -0.9915,  0.1114,  1.0360]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.5958,  0.9143, -0.8302,  ..., -0.0795,  0.0356,  0.3544],\n",
       "         [ 0.6664,  0.8663, -0.8206,  ..., -0.0660, -0.0228,  0.3231],\n",
       "         [ 0.6616,  0.8829, -0.8320,  ..., -0.0452, -0.0994,  0.2967],\n",
       "         ...,\n",
       "         [ 0.3205, -0.6818, -0.0454,  ..., -0.6727, -0.7787,  1.1092],\n",
       "         [ 1.3218, -0.7020, -1.0760,  ..., -0.9728, -0.2405,  0.8265],\n",
       "         [ 0.2434, -0.3605, -0.8424,  ..., -0.0908, -0.1475,  0.6281]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 1.4764,  2.5515, -0.8054,  ..., -0.7408,  0.5854, -2.2659],\n",
       "         [ 1.6141,  2.4739, -0.7761,  ..., -0.7215,  0.4234, -2.3880],\n",
       "         [ 1.5958,  2.4999, -0.7793,  ..., -0.6517,  0.2706, -2.4049],\n",
       "         ...,\n",
       "         [ 3.8463,  0.2613, -1.5299,  ..., -2.6709, -0.2188,  0.2802],\n",
       "         [ 5.7440, -0.2734, -3.4873,  ..., -3.3266,  0.4171,  0.4994],\n",
       "         [ 3.7563, -0.2951, -2.6874,  ..., -1.8446,  0.5208,  0.9403]]],\n",
       "       grad_fn=<PermuteBackward>)), attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "          [ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "          [ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "          ...,\n",
       "          [ 0.0540,  0.0375,  0.0332,  ...,  0.0201, -0.0480,  0.0724],\n",
       "          [ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520],\n",
       "          [ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.9156,  1.0634, -0.2741,  ..., -0.1662, -0.1363, -0.8428],\n",
       "          [ 0.9461,  1.0958, -0.2464,  ..., -0.1756, -0.1703, -0.8440],\n",
       "          [ 0.9758,  1.1076, -0.2352,  ..., -0.1836, -0.1847, -0.8192],\n",
       "          ...,\n",
       "          [ 0.8328,  0.5979,  0.6304,  ..., -0.4727, -1.2793,  1.6664],\n",
       "          [ 1.0250, -0.4521, -1.5119,  ..., -0.0787,  0.4344,  0.7233],\n",
       "          [ 0.1938,  0.1017, -2.3535,  ..., -0.5091, -0.1052,  1.6627]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.3933,  0.3043, -0.2023,  ...,  0.1985,  0.2346, -0.5402],\n",
       "          [ 0.4375,  0.3185, -0.1855,  ...,  0.1863,  0.1941, -0.5373],\n",
       "          [ 0.4548,  0.3255, -0.1658,  ...,  0.1743,  0.1989, -0.5231],\n",
       "          ...,\n",
       "          [-0.0949,  0.1630,  0.7266,  ...,  0.1444, -1.3562,  1.3974],\n",
       "          [ 0.3575, -0.6885, -1.4503,  ..., -0.9342,  0.5952,  0.3722],\n",
       "          [-0.0512, -0.6750, -2.2458,  ..., -0.7432,  0.3301,  1.2587]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.2071,  0.9489, -0.6494,  ...,  0.9213, -0.2973, -0.9567],\n",
       "          [ 0.2374,  0.9585, -0.6248,  ...,  0.9006, -0.3489, -0.9238],\n",
       "          [ 0.2464,  0.9809, -0.6202,  ...,  0.8855, -0.3586, -0.8963],\n",
       "          ...,\n",
       "          [-0.7476,  0.6471,  0.5672,  ..., -0.3418, -1.2544,  0.5400],\n",
       "          [-0.1788,  0.2359, -1.5412,  ..., -0.8350,  0.2809, -0.4816],\n",
       "          [-0.2697, -0.3103, -2.6733,  ..., -0.4990, -0.2893,  0.5538]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 1.4693,  0.0391, -0.7497,  ...,  1.5123, -0.5623, -0.5832],\n",
       "          [ 1.5013,  0.0470, -0.7449,  ...,  1.5048, -0.6189, -0.5611],\n",
       "          [ 1.5103,  0.0562, -0.7554,  ...,  1.4908, -0.6479, -0.5522],\n",
       "          ...,\n",
       "          [-0.5714, -0.2590,  0.1343,  ...,  0.4249, -0.8150,  0.3106],\n",
       "          [ 0.5622, -0.4188, -1.8813,  ..., -0.0362, -0.3020, -0.0747],\n",
       "          [ 0.7146, -0.7559, -2.6028,  ...,  0.1587, -0.5205,  0.1761]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 1.3464,  0.4357, -0.9102,  ...,  0.5386, -1.3498, -1.5001],\n",
       "          [ 1.3908,  0.4392, -0.9228,  ...,  0.5474, -1.3786, -1.4790],\n",
       "          [ 1.4049,  0.4384, -0.9358,  ...,  0.5331, -1.3844, -1.4777],\n",
       "          ...,\n",
       "          [-1.3468,  0.6591,  0.5007,  ..., -0.4910, -0.7752, -0.1183],\n",
       "          [ 0.3819,  0.3082, -1.4197,  ...,  0.0685, -0.3215, -0.7804],\n",
       "          [ 0.5596, -0.0037, -2.2351,  ...,  0.0812, -0.5723, -0.3554]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 1.0619,  0.6954, -2.0038,  ..., -0.3962, -1.9340, -1.0415],\n",
       "          [ 1.1375,  0.7019, -1.9848,  ..., -0.3725, -2.0207, -0.9980],\n",
       "          [ 1.2100,  0.6881, -1.9637,  ..., -0.3556, -2.0325, -1.0247],\n",
       "          ...,\n",
       "          [-1.4129,  0.9158,  0.6443,  ..., -0.9858, -1.9156,  0.5450],\n",
       "          [ 0.5878,  0.1117, -1.9039,  ..., -0.2968, -0.6944, -0.0989],\n",
       "          [ 0.0981, -0.2018, -2.8803,  ..., -0.3664, -0.9444,  0.1085]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.6015,  1.1536, -1.6290,  ..., -0.6842, -1.4619, -0.5704],\n",
       "          [ 0.6530,  1.1733, -1.6237,  ..., -0.6925, -1.5167, -0.5520],\n",
       "          [ 0.6745,  1.1747, -1.6395,  ..., -0.6756, -1.5211, -0.5644],\n",
       "          ...,\n",
       "          [-0.9452,  1.1977,  1.3949,  ..., -1.0470, -0.7192,  1.2919],\n",
       "          [ 0.7428,  1.1699, -1.1035,  ..., -1.0125, -0.3954,  0.8313],\n",
       "          [-0.2416,  1.0022, -1.9961,  ..., -0.9663, -0.4888,  0.9290]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.8313,  1.3704, -1.9395,  ..., -0.0134, -1.8234, -0.5403],\n",
       "          [ 0.9062,  1.3974, -1.9473,  ..., -0.0270, -1.8912, -0.4990],\n",
       "          [ 0.9857,  1.4109, -1.9736,  ..., -0.0311, -1.9189, -0.5002],\n",
       "          ...,\n",
       "          [-0.5132,  1.5491,  0.5664,  ..., -0.8778, -1.2237,  0.7145],\n",
       "          [ 1.4038,  1.0113, -1.2193,  ..., -1.1647, -0.5987, -0.3652],\n",
       "          [-0.1021,  1.2665, -2.0035,  ..., -0.8056, -0.1584, -0.0238]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.6594,  0.8182, -2.0167,  ..., -0.5913, -0.6711, -0.3155],\n",
       "          [ 0.7234,  0.7991, -1.9943,  ..., -0.6506, -0.7660, -0.2838],\n",
       "          [ 0.7489,  0.8035, -2.0283,  ..., -0.6751, -0.8198, -0.2729],\n",
       "          ...,\n",
       "          [-0.7829,  0.5609,  0.3888,  ..., -1.3396, -0.7099,  1.1600],\n",
       "          [ 0.9701,  0.0308, -1.4273,  ..., -1.8822,  0.3705, -0.0501],\n",
       "          [ 0.0121,  0.6457, -1.9682,  ..., -0.9920,  0.6946,  0.0159]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.7801,  0.9696, -2.0823,  ..., -0.8309, -0.3148,  0.0946],\n",
       "          [ 0.8911,  0.9265, -2.0851,  ..., -0.8720, -0.3838,  0.0760],\n",
       "          [ 0.9231,  0.9462, -2.1496,  ..., -0.8830, -0.4642,  0.0740],\n",
       "          ...,\n",
       "          [-0.0418,  0.4412,  0.4693,  ..., -1.4395, -1.1413,  1.8067],\n",
       "          [ 1.3913,  0.0687, -1.5188,  ..., -2.0519, -0.1029,  1.2681],\n",
       "          [ 0.3388,  0.8382, -1.4421,  ..., -0.9915,  0.1114,  1.0360]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.5958,  0.9143, -0.8302,  ..., -0.0795,  0.0356,  0.3544],\n",
       "          [ 0.6664,  0.8663, -0.8206,  ..., -0.0660, -0.0228,  0.3231],\n",
       "          [ 0.6616,  0.8829, -0.8320,  ..., -0.0452, -0.0994,  0.2967],\n",
       "          ...,\n",
       "          [ 0.3205, -0.6818, -0.0454,  ..., -0.6727, -0.7787,  1.1092],\n",
       "          [ 1.3218, -0.7020, -1.0760,  ..., -0.9728, -0.2405,  0.8265],\n",
       "          [ 0.2434, -0.3605, -0.8424,  ..., -0.0908, -0.1475,  0.6281]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 1.4764,  2.5515, -0.8054,  ..., -0.7408,  0.5854, -2.2659],\n",
       "          [ 1.6141,  2.4739, -0.7761,  ..., -0.7215,  0.4234, -2.3880],\n",
       "          [ 1.5958,  2.4999, -0.7793,  ..., -0.6517,  0.2706, -2.4049],\n",
       "          ...,\n",
       "          [ 3.8463,  0.2613, -1.5299,  ..., -2.6709, -0.2188,  0.2802],\n",
       "          [ 5.7440, -0.2734, -3.4873,  ..., -3.3266,  0.4171,  0.4994],\n",
       "          [ 3.7563, -0.2951, -2.6874,  ..., -1.8446,  0.5208,  0.9403]]],\n",
       "        grad_fn=<PermuteBackward>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
