{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "The task is from [aidea](https://aidea-web.tw/topic/c4a666bb-7d83-45a6-8c3b-57514faf2901), the goal is to predict the sentiment of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41411</td>\n",
       "      <td>I watched this film because I'm a big fan of R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37586</td>\n",
       "      <td>It does not seem that this movie managed to pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6017</td>\n",
       "      <td>Enough is not a bad movie , just mediocre .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44656</td>\n",
       "      <td>my friend and i rented this one a few nights a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38711</td>\n",
       "      <td>Just about everything in this movie is wrong, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>8019</td>\n",
       "      <td>It 's one of the most honest films ever made a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29337</th>\n",
       "      <td>453</td>\n",
       "      <td>An absorbing and unsettling psychological drama .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29338</th>\n",
       "      <td>13097</td>\n",
       "      <td>Soylent Green IS...a really good movie, actual...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>26896</td>\n",
       "      <td>There just isn't enough here. There a few funn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29340</th>\n",
       "      <td>27094</td>\n",
       "      <td>This show was absolutely terrible. For one Geo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29341 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                             review  sentiment\n",
       "0      41411  I watched this film because I'm a big fan of R...          0\n",
       "1      37586  It does not seem that this movie managed to pl...          1\n",
       "2       6017        Enough is not a bad movie , just mediocre .          0\n",
       "3      44656  my friend and i rented this one a few nights a...          0\n",
       "4      38711  Just about everything in this movie is wrong, ...          0\n",
       "...      ...                                                ...        ...\n",
       "29336   8019  It 's one of the most honest films ever made a...          1\n",
       "29337    453  An absorbing and unsettling psychological drama .          1\n",
       "29338  13097  Soylent Green IS...a really good movie, actual...          1\n",
       "29339  26896  There just isn't enough here. There a few funn...          0\n",
       "29340  27094  This show was absolutely terrible. For one Geo...          0\n",
       "\n",
       "[29341 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41411</td>\n",
       "      <td>I watched this film because I'm a big fan of R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37586</td>\n",
       "      <td>It does not seem that this movie managed to pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6017</td>\n",
       "      <td>Enough is not a bad movie , just mediocre .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44656</td>\n",
       "      <td>my friend and i rented this one a few nights a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38711</td>\n",
       "      <td>Just about everything in this movie is wrong, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                             review  sentiment\n",
       "0  41411  I watched this film because I'm a big fan of R...          0\n",
       "1  37586  It does not seem that this movie managed to pl...          1\n",
       "2   6017        Enough is not a bad movie , just mediocre .          0\n",
       "3  44656  my friend and i rented this one a few nights a...          0\n",
       "4  38711  Just about everything in this movie is wrong, ...          0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['len'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = list()\n",
    "\n",
    "for text in train_df['review']:\n",
    "    lens += [len(tokenizer(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['len'] = lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    957\n",
       "0    830\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['len'] > 600]['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 203),\n",
       " (15, 189),\n",
       " (19, 186),\n",
       " (18, 184),\n",
       " (17, 183),\n",
       " (21, 169),\n",
       " (153, 162),\n",
       " (147, 162),\n",
       " (143, 161),\n",
       " (152, 160),\n",
       " (13, 158),\n",
       " (24, 158),\n",
       " (144, 157),\n",
       " (150, 156),\n",
       " (20, 153),\n",
       " (22, 153),\n",
       " (162, 152),\n",
       " (12, 151),\n",
       " (14, 150),\n",
       " (23, 149),\n",
       " (160, 149),\n",
       " (148, 148),\n",
       " (165, 147),\n",
       " (25, 147),\n",
       " (139, 146),\n",
       " (146, 145),\n",
       " (151, 142),\n",
       " (142, 141),\n",
       " (158, 141),\n",
       " (10, 139),\n",
       " (149, 139),\n",
       " (136, 139),\n",
       " (11, 139),\n",
       " (154, 138),\n",
       " (163, 137),\n",
       " (159, 136),\n",
       " (145, 136),\n",
       " (156, 132),\n",
       " (164, 131),\n",
       " (27, 130),\n",
       " (9, 128),\n",
       " (135, 127),\n",
       " (161, 127),\n",
       " (170, 126),\n",
       " (141, 125),\n",
       " (140, 123),\n",
       " (179, 122),\n",
       " (157, 120),\n",
       " (138, 120),\n",
       " (26, 120),\n",
       " (169, 119),\n",
       " (28, 118),\n",
       " (172, 117),\n",
       " (167, 116),\n",
       " (155, 116),\n",
       " (178, 116),\n",
       " (171, 116),\n",
       " (194, 114),\n",
       " (168, 114),\n",
       " (177, 114),\n",
       " (174, 113),\n",
       " (131, 113),\n",
       " (130, 111),\n",
       " (134, 111),\n",
       " (8, 109),\n",
       " (175, 109),\n",
       " (186, 108),\n",
       " (183, 106),\n",
       " (187, 106),\n",
       " (133, 104),\n",
       " (184, 104),\n",
       " (6, 102),\n",
       " (137, 101),\n",
       " (198, 100),\n",
       " (190, 100),\n",
       " (176, 100),\n",
       " (185, 99),\n",
       " (166, 99),\n",
       " (132, 98),\n",
       " (173, 98),\n",
       " (180, 97),\n",
       " (123, 95),\n",
       " (200, 95),\n",
       " (127, 95),\n",
       " (31, 94),\n",
       " (128, 94),\n",
       " (30, 93),\n",
       " (126, 93),\n",
       " (189, 93),\n",
       " (192, 92),\n",
       " (191, 91),\n",
       " (196, 90),\n",
       " (193, 89),\n",
       " (7, 89),\n",
       " (29, 89),\n",
       " (125, 89),\n",
       " (195, 88),\n",
       " (182, 88),\n",
       " (205, 88),\n",
       " (210, 86),\n",
       " (129, 86),\n",
       " (188, 85),\n",
       " (120, 85),\n",
       " (197, 84),\n",
       " (204, 84),\n",
       " (218, 83),\n",
       " (214, 82),\n",
       " (215, 81),\n",
       " (209, 81),\n",
       " (211, 81),\n",
       " (119, 80),\n",
       " (206, 80),\n",
       " (203, 80),\n",
       " (35, 80),\n",
       " (181, 79),\n",
       " (207, 79),\n",
       " (216, 79),\n",
       " (220, 78),\n",
       " (202, 77),\n",
       " (33, 77),\n",
       " (217, 76),\n",
       " (219, 74),\n",
       " (32, 74),\n",
       " (124, 74),\n",
       " (231, 73),\n",
       " (213, 72),\n",
       " (199, 71),\n",
       " (230, 71),\n",
       " (201, 70),\n",
       " (226, 69),\n",
       " (239, 69),\n",
       " (121, 69),\n",
       " (225, 68),\n",
       " (245, 68),\n",
       " (237, 67),\n",
       " (222, 66),\n",
       " (236, 66),\n",
       " (34, 64),\n",
       " (36, 64),\n",
       " (227, 63),\n",
       " (242, 63),\n",
       " (238, 62),\n",
       " (221, 62),\n",
       " (115, 62),\n",
       " (5, 61),\n",
       " (235, 61),\n",
       " (208, 60),\n",
       " (233, 59),\n",
       " (262, 59),\n",
       " (4, 59),\n",
       " (258, 58),\n",
       " (118, 58),\n",
       " (248, 57),\n",
       " (122, 57),\n",
       " (223, 57),\n",
       " (228, 56),\n",
       " (87, 56),\n",
       " (256, 56),\n",
       " (267, 56),\n",
       " (86, 55),\n",
       " (229, 55),\n",
       " (249, 55),\n",
       " (260, 54),\n",
       " (241, 54),\n",
       " (224, 54),\n",
       " (212, 54),\n",
       " (244, 54),\n",
       " (243, 54),\n",
       " (273, 53),\n",
       " (113, 53),\n",
       " (38, 53),\n",
       " (70, 53),\n",
       " (254, 53),\n",
       " (282, 52),\n",
       " (261, 52),\n",
       " (246, 52),\n",
       " (232, 52),\n",
       " (56, 52),\n",
       " (269, 52),\n",
       " (117, 51),\n",
       " (105, 51),\n",
       " (253, 51),\n",
       " (68, 50),\n",
       " (61, 50),\n",
       " (112, 50),\n",
       " (100, 49),\n",
       " (62, 49),\n",
       " (251, 49),\n",
       " (234, 49),\n",
       " (268, 49),\n",
       " (103, 49),\n",
       " (116, 49),\n",
       " (252, 49),\n",
       " (259, 49),\n",
       " (114, 49),\n",
       " (295, 48),\n",
       " (92, 48),\n",
       " (108, 48),\n",
       " (65, 48),\n",
       " (284, 48),\n",
       " (283, 48),\n",
       " (93, 48),\n",
       " (84, 48),\n",
       " (300, 47),\n",
       " (278, 46),\n",
       " (271, 46),\n",
       " (73, 46),\n",
       " (71, 45),\n",
       " (287, 45),\n",
       " (240, 45),\n",
       " (276, 44),\n",
       " (102, 44),\n",
       " (281, 44),\n",
       " (58, 44),\n",
       " (60, 44),\n",
       " (110, 44),\n",
       " (104, 44),\n",
       " (82, 44),\n",
       " (90, 43),\n",
       " (364, 43),\n",
       " (69, 43),\n",
       " (64, 43),\n",
       " (288, 43),\n",
       " (72, 43),\n",
       " (94, 42),\n",
       " (76, 42),\n",
       " (291, 42),\n",
       " (95, 42),\n",
       " (66, 42),\n",
       " (247, 42),\n",
       " (250, 42),\n",
       " (77, 42),\n",
       " (107, 42),\n",
       " (74, 42),\n",
       " (264, 42),\n",
       " (101, 41),\n",
       " (266, 41),\n",
       " (37, 41),\n",
       " (47, 41),\n",
       " (263, 41),\n",
       " (309, 40),\n",
       " (350, 40),\n",
       " (270, 40),\n",
       " (55, 40),\n",
       " (83, 40),\n",
       " (314, 40),\n",
       " (111, 40),\n",
       " (96, 39),\n",
       " (280, 39),\n",
       " (304, 39),\n",
       " (67, 39),\n",
       " (296, 38),\n",
       " (289, 38),\n",
       " (3, 38),\n",
       " (91, 38),\n",
       " (79, 38),\n",
       " (272, 37),\n",
       " (294, 37),\n",
       " (332, 37),\n",
       " (257, 37),\n",
       " (277, 37),\n",
       " (97, 37),\n",
       " (255, 37),\n",
       " (322, 37),\n",
       " (51, 37),\n",
       " (279, 37),\n",
       " (59, 36),\n",
       " (109, 36),\n",
       " (320, 36),\n",
       " (88, 36),\n",
       " (305, 36),\n",
       " (301, 36),\n",
       " (53, 36),\n",
       " (327, 36),\n",
       " (333, 36),\n",
       " (40, 36),\n",
       " (275, 36),\n",
       " (265, 36),\n",
       " (98, 36),\n",
       " (331, 36),\n",
       " (78, 36),\n",
       " (308, 35),\n",
       " (323, 35),\n",
       " (75, 35),\n",
       " (298, 35),\n",
       " (316, 35),\n",
       " (325, 34),\n",
       " (39, 34),\n",
       " (80, 34),\n",
       " (349, 34),\n",
       " (85, 34),\n",
       " (285, 33),\n",
       " (338, 33),\n",
       " (310, 33),\n",
       " (311, 33),\n",
       " (63, 33),\n",
       " (54, 33),\n",
       " (319, 33),\n",
       " (306, 33),\n",
       " (274, 33),\n",
       " (336, 33),\n",
       " (57, 32),\n",
       " (362, 32),\n",
       " (290, 32),\n",
       " (345, 32),\n",
       " (371, 32),\n",
       " (81, 32),\n",
       " (293, 32),\n",
       " (106, 32),\n",
       " (324, 31),\n",
       " (330, 31),\n",
       " (328, 31),\n",
       " (337, 31),\n",
       " (44, 31),\n",
       " (340, 31),\n",
       " (52, 31),\n",
       " (292, 30),\n",
       " (335, 30),\n",
       " (303, 30),\n",
       " (46, 30),\n",
       " (43, 29),\n",
       " (334, 29),\n",
       " (393, 29),\n",
       " (315, 29),\n",
       " (286, 29),\n",
       " (354, 29),\n",
       " (89, 28),\n",
       " (45, 28),\n",
       " (407, 28),\n",
       " (297, 28),\n",
       " (302, 28),\n",
       " (343, 28),\n",
       " (369, 28),\n",
       " (326, 28),\n",
       " (42, 28),\n",
       " (339, 27),\n",
       " (370, 27),\n",
       " (417, 27),\n",
       " (360, 27),\n",
       " (49, 27),\n",
       " (307, 27),\n",
       " (347, 27),\n",
       " (414, 27),\n",
       " (313, 27),\n",
       " (341, 27),\n",
       " (367, 26),\n",
       " (361, 26),\n",
       " (50, 26),\n",
       " (329, 26),\n",
       " (411, 26),\n",
       " (346, 26),\n",
       " (395, 26),\n",
       " (357, 26),\n",
       " (342, 26),\n",
       " (321, 25),\n",
       " (317, 25),\n",
       " (427, 25),\n",
       " (348, 25),\n",
       " (99, 25),\n",
       " (356, 25),\n",
       " (318, 25),\n",
       " (355, 25),\n",
       " (373, 25),\n",
       " (352, 25),\n",
       " (299, 24),\n",
       " (379, 24),\n",
       " (351, 24),\n",
       " (372, 24),\n",
       " (312, 23),\n",
       " (48, 23),\n",
       " (397, 23),\n",
       " (380, 23),\n",
       " (353, 22),\n",
       " (41, 22),\n",
       " (384, 22),\n",
       " (386, 22),\n",
       " (389, 22),\n",
       " (433, 22),\n",
       " (400, 22),\n",
       " (368, 22),\n",
       " (453, 22),\n",
       " (436, 22),\n",
       " (363, 21),\n",
       " (391, 21),\n",
       " (499, 21),\n",
       " (358, 21),\n",
       " (404, 21),\n",
       " (430, 21),\n",
       " (383, 21),\n",
       " (405, 20),\n",
       " (365, 20),\n",
       " (484, 20),\n",
       " (431, 20),\n",
       " (454, 20),\n",
       " (428, 20),\n",
       " (403, 20),\n",
       " (382, 20),\n",
       " (408, 20),\n",
       " (465, 20),\n",
       " (374, 20),\n",
       " (378, 20),\n",
       " (439, 20),\n",
       " (344, 20),\n",
       " (401, 20),\n",
       " (396, 20),\n",
       " (456, 20),\n",
       " (394, 19),\n",
       " (402, 19),\n",
       " (366, 19),\n",
       " (419, 19),\n",
       " (410, 19),\n",
       " (418, 19),\n",
       " (468, 19),\n",
       " (376, 19),\n",
       " (464, 19),\n",
       " (460, 19),\n",
       " (381, 19),\n",
       " (387, 19),\n",
       " (392, 19),\n",
       " (461, 18),\n",
       " (385, 18),\n",
       " (375, 18),\n",
       " (478, 18),\n",
       " (451, 18),\n",
       " (458, 18),\n",
       " (498, 18),\n",
       " (445, 18),\n",
       " (388, 18),\n",
       " (505, 18),\n",
       " (398, 18),\n",
       " (415, 17),\n",
       " (575, 17),\n",
       " (377, 17),\n",
       " (423, 17),\n",
       " (452, 17),\n",
       " (503, 17),\n",
       " (437, 17),\n",
       " (438, 17),\n",
       " (399, 17),\n",
       " (448, 16),\n",
       " (550, 16),\n",
       " (359, 16),\n",
       " (457, 16),\n",
       " (459, 16),\n",
       " (425, 16),\n",
       " (474, 16),\n",
       " (409, 16),\n",
       " (489, 16),\n",
       " (390, 16),\n",
       " (537, 15),\n",
       " (485, 15),\n",
       " (496, 15),\n",
       " (476, 15),\n",
       " (541, 15),\n",
       " (412, 15),\n",
       " (637, 15),\n",
       " (554, 15),\n",
       " (422, 15),\n",
       " (487, 15),\n",
       " (434, 15),\n",
       " (491, 15),\n",
       " (556, 15),\n",
       " (488, 15),\n",
       " (446, 15),\n",
       " (471, 15),\n",
       " (645, 15),\n",
       " (506, 14),\n",
       " (443, 14),\n",
       " (521, 14),\n",
       " (450, 14),\n",
       " (566, 14),\n",
       " (432, 14),\n",
       " (526, 14),\n",
       " (470, 14),\n",
       " (421, 14),\n",
       " (477, 14),\n",
       " (447, 14),\n",
       " (551, 14),\n",
       " (2, 14),\n",
       " (426, 14),\n",
       " (513, 13),\n",
       " (486, 13),\n",
       " (483, 13),\n",
       " (441, 13),\n",
       " (479, 13),\n",
       " (435, 13),\n",
       " (444, 13),\n",
       " (429, 13),\n",
       " (472, 13),\n",
       " (542, 13),\n",
       " (511, 13),\n",
       " (510, 13),\n",
       " (475, 13),\n",
       " (504, 13),\n",
       " (558, 13),\n",
       " (416, 13),\n",
       " (420, 13),\n",
       " (507, 13),\n",
       " (473, 12),\n",
       " (480, 12),\n",
       " (535, 12),\n",
       " (540, 12),\n",
       " (497, 12),\n",
       " (517, 12),\n",
       " (514, 12),\n",
       " (626, 12),\n",
       " (494, 12),\n",
       " (406, 12),\n",
       " (413, 12),\n",
       " (502, 12),\n",
       " (515, 12),\n",
       " (625, 12),\n",
       " (616, 12),\n",
       " (559, 12),\n",
       " (493, 12),\n",
       " (463, 12),\n",
       " (440, 11),\n",
       " (660, 11),\n",
       " (586, 11),\n",
       " (615, 11),\n",
       " (603, 11),\n",
       " (520, 11),\n",
       " (524, 11),\n",
       " (567, 11),\n",
       " (583, 11),\n",
       " (565, 11),\n",
       " (618, 11),\n",
       " (481, 11),\n",
       " (576, 10),\n",
       " (666, 10),\n",
       " (532, 10),\n",
       " (624, 10),\n",
       " (527, 10),\n",
       " (561, 10),\n",
       " (619, 10),\n",
       " (599, 10),\n",
       " (482, 10),\n",
       " (652, 10),\n",
       " (664, 10),\n",
       " (466, 10),\n",
       " (572, 10),\n",
       " (500, 10),\n",
       " (578, 10),\n",
       " (509, 10),\n",
       " (620, 10),\n",
       " (519, 10),\n",
       " (674, 9),\n",
       " (798, 9),\n",
       " (490, 9),\n",
       " (585, 9),\n",
       " (442, 9),\n",
       " (730, 9),\n",
       " (632, 9),\n",
       " (607, 9),\n",
       " (557, 9),\n",
       " (731, 9),\n",
       " (587, 9),\n",
       " (495, 9),\n",
       " (530, 9),\n",
       " (574, 9),\n",
       " (657, 9),\n",
       " (728, 9),\n",
       " (594, 9),\n",
       " (589, 9),\n",
       " (608, 9),\n",
       " (545, 9),\n",
       " (518, 9),\n",
       " (609, 9),\n",
       " (595, 9),\n",
       " (549, 9),\n",
       " (536, 9),\n",
       " (462, 8),\n",
       " (614, 8),\n",
       " (764, 8),\n",
       " (562, 8),\n",
       " (501, 8),\n",
       " (544, 8),\n",
       " (512, 8),\n",
       " (533, 8),\n",
       " (424, 8),\n",
       " (571, 8),\n",
       " (647, 8),\n",
       " (539, 8),\n",
       " (659, 8),\n",
       " (661, 8),\n",
       " (705, 8),\n",
       " (522, 8),\n",
       " (467, 8),\n",
       " (596, 8),\n",
       " (548, 8),\n",
       " (628, 8),\n",
       " (742, 8),\n",
       " (593, 8),\n",
       " (546, 8),\n",
       " (751, 8),\n",
       " (610, 8),\n",
       " (623, 8),\n",
       " (525, 8),\n",
       " (516, 8),\n",
       " (752, 8),\n",
       " (703, 8),\n",
       " (555, 8),\n",
       " (590, 8),\n",
       " (508, 8),\n",
       " (469, 8),\n",
       " (492, 7),\n",
       " (756, 7),\n",
       " (668, 7),\n",
       " (694, 7),\n",
       " (622, 7),\n",
       " (718, 7),\n",
       " (597, 7),\n",
       " (654, 7),\n",
       " (573, 7),\n",
       " (577, 7),\n",
       " (681, 7),\n",
       " (662, 7),\n",
       " (621, 7),\n",
       " (692, 7),\n",
       " (762, 7),\n",
       " (528, 7),\n",
       " (523, 7),\n",
       " (655, 7),\n",
       " (667, 7),\n",
       " (648, 7),\n",
       " (604, 7),\n",
       " (455, 7),\n",
       " (598, 7),\n",
       " (683, 7),\n",
       " (633, 7),\n",
       " (883, 7),\n",
       " (685, 7),\n",
       " (651, 7),\n",
       " (790, 7),\n",
       " (759, 7),\n",
       " (581, 7),\n",
       " (801, 7),\n",
       " (631, 7),\n",
       " (809, 7),\n",
       " (543, 7),\n",
       " (691, 7),\n",
       " (947, 7),\n",
       " (634, 7),\n",
       " (663, 7),\n",
       " (560, 7),\n",
       " (693, 7),\n",
       " (569, 7),\n",
       " (653, 7),\n",
       " (582, 7),\n",
       " (570, 7),\n",
       " (697, 7),\n",
       " (866, 6),\n",
       " (706, 6),\n",
       " (804, 6),\n",
       " (702, 6),\n",
       " (723, 6),\n",
       " (1103, 6),\n",
       " (760, 6),\n",
       " (833, 6),\n",
       " (568, 6),\n",
       " (680, 6),\n",
       " (689, 6),\n",
       " (553, 6),\n",
       " (534, 6),\n",
       " (640, 6),\n",
       " (673, 6),\n",
       " (563, 6),\n",
       " (787, 6),\n",
       " (612, 6),\n",
       " (700, 6),\n",
       " (690, 6),\n",
       " (547, 6),\n",
       " (991, 6),\n",
       " (627, 6),\n",
       " (672, 6),\n",
       " (817, 6),\n",
       " (720, 6),\n",
       " (678, 6),\n",
       " (682, 6),\n",
       " (650, 6),\n",
       " (722, 6),\n",
       " (579, 6),\n",
       " (636, 6),\n",
       " (701, 6),\n",
       " (613, 6),\n",
       " (538, 6),\n",
       " (531, 6),\n",
       " (749, 6),\n",
       " (882, 5),\n",
       " (861, 5),\n",
       " (781, 5),\n",
       " (564, 5),\n",
       " (641, 5),\n",
       " (617, 5),\n",
       " (638, 5),\n",
       " (580, 5),\n",
       " (775, 5),\n",
       " (851, 5),\n",
       " (629, 5),\n",
       " (646, 5),\n",
       " (687, 5),\n",
       " (611, 5),\n",
       " (773, 5),\n",
       " (840, 5),\n",
       " (676, 5),\n",
       " (715, 5),\n",
       " (828, 5),\n",
       " (713, 5),\n",
       " (776, 5),\n",
       " (875, 5),\n",
       " (584, 5),\n",
       " (449, 5),\n",
       " (606, 5),\n",
       " (955, 5),\n",
       " (839, 5),\n",
       " (602, 5),\n",
       " (900, 5),\n",
       " (707, 5),\n",
       " (688, 5),\n",
       " (795, 5),\n",
       " (743, 5),\n",
       " (887, 5),\n",
       " (1098, 5),\n",
       " (872, 5),\n",
       " (727, 5),\n",
       " (677, 4),\n",
       " (669, 4),\n",
       " (803, 4),\n",
       " (843, 4),\n",
       " (766, 4),\n",
       " (658, 4),\n",
       " (885, 4),\n",
       " (644, 4),\n",
       " (769, 4),\n",
       " (919, 4),\n",
       " (771, 4),\n",
       " (600, 4),\n",
       " (788, 4),\n",
       " (1173, 4),\n",
       " (886, 4),\n",
       " (898, 4),\n",
       " (737, 4),\n",
       " (1152, 4),\n",
       " (1100, 4),\n",
       " (862, 4),\n",
       " (746, 4),\n",
       " (1099, 4),\n",
       " (639, 4),\n",
       " (874, 4),\n",
       " (984, 4),\n",
       " (782, 4),\n",
       " (814, 4),\n",
       " (768, 4),\n",
       " (1040, 4),\n",
       " (605, 4),\n",
       " (745, 4),\n",
       " (925, 4),\n",
       " (818, 4),\n",
       " (761, 4),\n",
       " (832, 4),\n",
       " (808, 4),\n",
       " (724, 4),\n",
       " (552, 4),\n",
       " (740, 4),\n",
       " (753, 4),\n",
       " (696, 4),\n",
       " (665, 4),\n",
       " (891, 4),\n",
       " (698, 4),\n",
       " (710, 4),\n",
       " (630, 4),\n",
       " (953, 4),\n",
       " (684, 4),\n",
       " (591, 4),\n",
       " (695, 4),\n",
       " (719, 4),\n",
       " (1051, 4),\n",
       " (732, 4),\n",
       " (792, 4),\n",
       " (763, 4),\n",
       " (852, 4),\n",
       " (1131, 4),\n",
       " (767, 4),\n",
       " (827, 4),\n",
       " (889, 4),\n",
       " (529, 4),\n",
       " (983, 4),\n",
       " (1121, 3),\n",
       " (750, 3),\n",
       " (820, 3),\n",
       " (796, 3),\n",
       " (1024, 3),\n",
       " (929, 3),\n",
       " (971, 3),\n",
       " (1036, 3),\n",
       " (954, 3),\n",
       " (1127, 3),\n",
       " (877, 3),\n",
       " (635, 3),\n",
       " (1114, 3),\n",
       " (725, 3),\n",
       " (910, 3),\n",
       " (797, 3),\n",
       " (1025, 3),\n",
       " (1018, 3),\n",
       " (793, 3),\n",
       " (1046, 3),\n",
       " (1085, 3),\n",
       " (914, 3),\n",
       " (721, 3),\n",
       " (899, 3),\n",
       " (906, 3),\n",
       " (1049, 3),\n",
       " (1021, 3),\n",
       " (1113, 3),\n",
       " (1061, 3),\n",
       " (932, 3),\n",
       " (778, 3),\n",
       " (855, 3),\n",
       " (850, 3),\n",
       " (811, 3),\n",
       " (757, 3),\n",
       " (709, 3),\n",
       " (835, 3),\n",
       " (643, 3),\n",
       " (987, 3),\n",
       " (592, 3),\n",
       " (913, 3),\n",
       " (588, 3),\n",
       " (729, 3),\n",
       " (829, 3),\n",
       " (1117, 3),\n",
       " (908, 3),\n",
       " (888, 3),\n",
       " (810, 3),\n",
       " (679, 3),\n",
       " (699, 3),\n",
       " (747, 3),\n",
       " (968, 3),\n",
       " (999, 3),\n",
       " (995, 3),\n",
       " (871, 3),\n",
       " (656, 3),\n",
       " (671, 3),\n",
       " (780, 3),\n",
       " (686, 3),\n",
       " (794, 3),\n",
       " (824, 3),\n",
       " (717, 3),\n",
       " (1172, 3),\n",
       " (1058, 3),\n",
       " (1116, 3),\n",
       " (890, 3),\n",
       " (1136, 3),\n",
       " (849, 3),\n",
       " (920, 3),\n",
       " (1037, 3),\n",
       " (892, 3),\n",
       " (1059, 3),\n",
       " (733, 3),\n",
       " (924, 3),\n",
       " (806, 3),\n",
       " (873, 3),\n",
       " (704, 3),\n",
       " (951, 3),\n",
       " (864, 3),\n",
       " (748, 3),\n",
       " (741, 3),\n",
       " (783, 3),\n",
       " (1163, 3),\n",
       " (791, 3),\n",
       " (736, 3),\n",
       " (876, 3),\n",
       " (848, 3),\n",
       " (975, 3),\n",
       " (941, 2),\n",
       " (957, 2),\n",
       " (670, 2),\n",
       " (949, 2),\n",
       " (1150, 2),\n",
       " (815, 2),\n",
       " (1048, 2),\n",
       " (1077, 2),\n",
       " (967, 2),\n",
       " (1081, 2),\n",
       " (867, 2),\n",
       " (1039, 2),\n",
       " (1129, 2),\n",
       " (870, 2),\n",
       " (1054, 2),\n",
       " (939, 2),\n",
       " (772, 2),\n",
       " (990, 2),\n",
       " (784, 2),\n",
       " (1134, 2),\n",
       " (1005, 2),\n",
       " (738, 2),\n",
       " (726, 2),\n",
       " (1112, 2),\n",
       " (1075, 2),\n",
       " (1092, 2),\n",
       " (909, 2),\n",
       " (962, 2),\n",
       " (755, 2),\n",
       " (1026, 2),\n",
       " (958, 2),\n",
       " (823, 2),\n",
       " (712, 2),\n",
       " (1034, 2),\n",
       " (915, 2),\n",
       " (1065, 2),\n",
       " (1102, 2),\n",
       " (859, 2),\n",
       " (923, 2),\n",
       " (1123, 2),\n",
       " (1064, 2),\n",
       " (1109, 2),\n",
       " (1023, 2),\n",
       " (834, 2),\n",
       " (1000, 2),\n",
       " (880, 2),\n",
       " (1080, 2),\n",
       " (831, 2),\n",
       " (1142, 2),\n",
       " (1071, 2),\n",
       " (946, 2),\n",
       " (869, 2),\n",
       " (758, 2),\n",
       " (734, 2),\n",
       " (1007, 2),\n",
       " (1093, 2),\n",
       " (1067, 2),\n",
       " (858, 2),\n",
       " (917, 2),\n",
       " (770, 2),\n",
       " (1043, 2),\n",
       " (865, 2),\n",
       " (1211, 2),\n",
       " (938, 2),\n",
       " (969, 2),\n",
       " (902, 2),\n",
       " (821, 2),\n",
       " (1122, 2),\n",
       " (937, 2),\n",
       " (893, 2),\n",
       " (970, 2),\n",
       " (1084, 2),\n",
       " (1009, 2),\n",
       " (830, 2),\n",
       " (857, 2),\n",
       " (1153, 2),\n",
       " (972, 2),\n",
       " (739, 2),\n",
       " (911, 2),\n",
       " (1050, 2),\n",
       " (994, 2),\n",
       " (1096, 2),\n",
       " (1068, 2),\n",
       " (1073, 2),\n",
       " (1144, 2),\n",
       " (816, 2),\n",
       " (826, 2),\n",
       " (1143, 2),\n",
       " (1087, 2),\n",
       " (642, 2),\n",
       " (1028, 2),\n",
       " (1045, 2),\n",
       " (952, 2),\n",
       " (802, 2),\n",
       " (1097, 2),\n",
       " (1184, 2),\n",
       " (774, 2),\n",
       " (716, 2),\n",
       " (777, 2),\n",
       " (1140, 2),\n",
       " (845, 2),\n",
       " (1016, 2),\n",
       " (1012, 2),\n",
       " (601, 2),\n",
       " (936, 2),\n",
       " (1108, 2),\n",
       " (884, 2),\n",
       " (708, 2),\n",
       " (1105, 2),\n",
       " (1106, 1),\n",
       " (998, 1),\n",
       " (1119, 1),\n",
       " (1247, 1),\n",
       " (868, 1),\n",
       " (1486, 1),\n",
       " (1223, 1),\n",
       " (1042, 1),\n",
       " (789, 1),\n",
       " (1187, 1),\n",
       " (1044, 1),\n",
       " (1901, 1),\n",
       " (1992, 1),\n",
       " (1083, 1),\n",
       " (979, 1),\n",
       " (930, 1),\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29341.000000</td>\n",
       "      <td>29341.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29348.411097</td>\n",
       "      <td>0.509662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17002.074346</td>\n",
       "      <td>0.499915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14564.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29348.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44162.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>58681.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID     sentiment\n",
       "count  29341.000000  29341.000000\n",
       "mean   29348.411097      0.509662\n",
       "std    17002.074346      0.499915\n",
       "min        4.000000      0.000000\n",
       "25%    14564.000000      0.000000\n",
       "50%    29348.000000      1.000000\n",
       "75%    44162.000000      1.000000\n",
       "max    58681.000000      1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14954\n",
       "0    14387\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22622</td>\n",
       "      <td>Robert Lansing plays a scientist experimenting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10162</td>\n",
       "      <td>Well I've enjoy this movie, even though someti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17468</td>\n",
       "      <td>First things first - though I believe Joel Sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42579</td>\n",
       "      <td>I watched this movie on the grounds that Amber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701</td>\n",
       "      <td>A certain sexiness underlines even the dullest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>30370</td>\n",
       "      <td>It is difficult to rate a writer/director's fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29337</th>\n",
       "      <td>18654</td>\n",
       "      <td>After watching this movie once, it quickly bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29338</th>\n",
       "      <td>47985</td>\n",
       "      <td>Even though i sat and watched the whole thing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>9866</td>\n",
       "      <td>Warning Spoilers following. Superb recreation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29340</th>\n",
       "      <td>35559</td>\n",
       "      <td>My, my, my: Peter Cushing and Donald Pleasance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                             review\n",
       "0      22622  Robert Lansing plays a scientist experimenting...\n",
       "1      10162  Well I've enjoy this movie, even though someti...\n",
       "2      17468  First things first - though I believe Joel Sch...\n",
       "3      42579  I watched this movie on the grounds that Amber...\n",
       "4        701  A certain sexiness underlines even the dullest...\n",
       "...      ...                                                ...\n",
       "29336  30370  It is difficult to rate a writer/director's fi...\n",
       "29337  18654  After watching this movie once, it quickly bec...\n",
       "29338  47985  Even though i sat and watched the whole thing,...\n",
       "29339   9866  Warning Spoilers following. Superb recreation ...\n",
       "29340  35559  My, my, my: Peter Cushing and Donald Pleasance...\n",
       "\n",
       "[29341 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>30370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29337</th>\n",
       "      <td>18654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29338</th>\n",
       "      <td>47985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>9866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29340</th>\n",
       "      <td>35559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  sentiment\n",
       "0      22622          1\n",
       "1      10162          1\n",
       "2      17468          1\n",
       "3      42579          1\n",
       "4        701          1\n",
       "...      ...        ...\n",
       "29336  30370          1\n",
       "29337  18654          1\n",
       "29338  47985          1\n",
       "29339   9866          1\n",
       "29340  35559          1\n",
       "\n",
       "[29341 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=123) # Define the split - into 2 folds \n",
    "kf.get_n_splits(train_df) # returns the number of splitting iterations in the cross-validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41411</td>\n",
       "      <td>I watched this film because I'm a big fan of R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37586</td>\n",
       "      <td>It does not seem that this movie managed to pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6017</td>\n",
       "      <td>Enough is not a bad movie , just mediocre .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                             review  sentiment\n",
       "0  41411  I watched this film because I'm a big fan of R...          0\n",
       "1  37586  It does not seem that this movie managed to pl...          1\n",
       "2   6017        Enough is not a bad movie , just mediocre .          0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0 1 2 3 5 6 7 8 9] TEST: [4]\n",
      "9 1\n",
      "TRAIN: [1 2 3 4 5 6 7 8 9] TEST: [0]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 5 6 8 9] TEST: [7]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 6 7 8 9] TEST: [5]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 5 6 7 9] TEST: [8]\n",
      "9 1\n",
      "TRAIN: [0 1 2 4 5 6 7 8 9] TEST: [3]\n",
      "9 1\n",
      "TRAIN: [0 2 3 4 5 6 7 8 9] TEST: [1]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 5 7 8 9] TEST: [6]\n",
      "9 1\n",
      "TRAIN: [0 1 2 3 4 5 6 7 8] TEST: [9]\n",
      "9 1\n",
      "TRAIN: [0 1 3 4 5 6 7 8 9] TEST: [2]\n",
      "9 1\n"
     ]
    }
   ],
   "source": [
    "X, y = train_df, train_df['sentiment'].to_numpy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_df['review']:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-bb3d408e025a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, counter, max_size, min_freq, specials, vectors, unk_init, vectors_cache, specials_first)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# in frequency order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# sort by frequency, then alphabetically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "Vocab([i[0] for i in counter.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x140640a30>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab(Counter(dict(counter.most_common(10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102969"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 241, 0, 1]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[i] for i in ['I', 'am', 'aaaaaaaaaaaaa', '<pad>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(X):\n",
    "    if isinstance(X, list):\n",
    "        return [[vocab[i] for i in tokenizer(text)] for text in X]\n",
    "    return [vocab[i] for i in tokenizer(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 241, 5, 57, 412, 36, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(\"I am a good boy! ADJISDAKSD unkqwjs <pad> <pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 241, 5, 57], [412, 36]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline([\"I am a good\", \"boy!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Data Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch, use_bag=False):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_text, _label) in batch:\n",
    "         label_list.append(_label)\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    if use_bag:\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        text_list = torch.cat(text_list)\n",
    "    else:\n",
    "        offsets = torch.tensor(offsets[1:], dtype=torch.int64)\n",
    "        text_list = pad_sequence(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "df = train_df.iloc[:100]\n",
    "train_iter = list(zip(df['review'], df['sentiment']))\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8]) torch.Size([513, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([513, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 0, 0, 0, 1, 0, 1]), tensor([[  13,   11,  197,  ...,   14,   13, 5130],\n",
      "        [ 295,  130,   10,  ...,  213,   33, 1483],\n",
      "        [  14,   29,   29,  ...,    9,    8,    3],\n",
      "        ...,\n",
      "        [   0,  272,    0,  ...,    0,    0,    0],\n",
      "        [   0,  319,    0,  ...,    0,    0,    0],\n",
      "        [   0,    3,    0,  ...,    0,    0,    0]]), tensor([210, 513,  10, 233, 237, 276,  94, 309]))\n",
      "torch.Size([8]) torch.Size([537, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([537, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 0, 1, 1, 1, 0, 0]), tensor([[   45,     2,    14,  ...,    51,  4700,    18],\n",
      "        [    2,  3725,   373,  ...,    26,    12,  3034],\n",
      "        [   87,    31,   119,  ...,   531,   732,    18],\n",
      "        ...,\n",
      "        [    0,     0, 85389,  ...,     0,     0,     0],\n",
      "        [    0,     0,   373,  ...,     0,     0,     0],\n",
      "        [    0,     0,     3,  ...,     0,     0,     0]]), tensor([154, 153, 537,  13, 198, 185, 462,  14]))\n",
      "torch.Size([8]) torch.Size([394, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([394, 8]) torch.Size([8])\n",
      "(tensor([1, 1, 0, 0, 0, 0, 1, 1]), tensor([[  95,   14,   80,  ...,   12,  474,    5],\n",
      "        [  90, 7142,    8,  ...,    2,    3, 5631],\n",
      "        [  31, 5908, 1451,  ..., 1871, 4915, 5907],\n",
      "        ...,\n",
      "        [   0,    0,   25,  ...,    0,    0,    0],\n",
      "        [   0,    0,  136,  ...,    0,    0,    0],\n",
      "        [   0,    0,   24,  ...,    0,    0,    0]]), tensor([285, 157, 394, 296, 238, 190,  17,  12]))\n",
      "torch.Size([8]) torch.Size([221, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([221, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 0, 0, 1, 0, 0, 1]), tensor([[   14,     2,    47,  ...,  1842,    13,    11],\n",
      "        [   23,   349, 11118,  ...,     4,   241,     9],\n",
      "        [   10,    77,     9,  ...,  1328,   742,    16],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,    19,     0,     0],\n",
      "        [    0,     0,     0,  ...,    78,     0,     0],\n",
      "        [    0,     0,     0,  ...,     3,     0,     0]]), tensor([149, 180, 215,  15, 217, 221,  96, 123]))\n",
      "torch.Size([8]) torch.Size([1121, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([1121, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 1, 1, 0, 1, 1, 0]), tensor([[2020,   70,   99,  ...,   54,   13,   46],\n",
      "        [3173,  109,   65,  ...,   11,  416,   17],\n",
      "        [   4,   56,  818,  ..., 1435,  120,    5],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]]), tensor([ 614,  292,  295,   57, 1121,   12,  272,  139]))\n",
      "torch.Size([8]) torch.Size([324, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([324, 8]) torch.Size([8])\n",
      "(tensor([0, 0, 1, 1, 0, 0, 1, 1]), tensor([[   20,    37,     5,  ...,    37,   639,   301],\n",
      "        [   99,   220,   964,  ...,    54,    78,   650],\n",
      "        [ 2996,    11, 61405,  ...,   231,   377,    49],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,     0,     3,     0],\n",
      "        [    0,     0,     0,  ...,     0,     3,     0],\n",
      "        [    0,     0,     0,  ...,     0,     3,     0]]), tensor([ 23,  89,  16,  30,  76, 291, 324, 278]))\n",
      "torch.Size([8]) torch.Size([282, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([282, 8]) torch.Size([8])\n",
      "(tensor([1, 1, 1, 1, 0, 1, 1, 1]), tensor([[    2,  3011,    11,  ...,     2,    76,    19],\n",
      "        [16978,  5179,     9,  ...,  3022,  2178,    38],\n",
      "        [   10,  7939,    16,  ...,   209,     2,     2],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0]]), tensor([149, 147, 185, 193, 282,  21, 160,  31]))\n",
      "torch.Size([8]) torch.Size([492, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([492, 8]) torch.Size([8])\n",
      "(tensor([1, 0, 1, 1, 0, 1, 1, 1]), tensor([[  209,    11,    18,  ...,    46,  8437,   584],\n",
      "        [  100,  1893,  2445,  ...,     9,    10, 76474],\n",
      "        [    4,     3, 50867,  ...,    16,     5,     4],\n",
      "        ...,\n",
      "        [    0,     0,    99,  ...,     0,     0,     0],\n",
      "        [    0,     0,   123,  ...,     0,     0,     0],\n",
      "        [    0,     0,     3,  ...,     0,     0,     0]]), tensor([ 23,  45, 492,  18, 176,  18, 164, 350]))\n",
      "torch.Size([8]) torch.Size([941, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([941, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 0, 0, 0, 1, 0, 1]), tensor([[   14,  9276,    14,  ...,  5080,    54,    11],\n",
      "        [ 1188,   581,   332,  ...,  3805,  1487,     9],\n",
      "        [   66, 46576,     9,  ...,    25,    53,    16],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,     0,     0,   104],\n",
      "        [    0,     0,     0,  ...,     0,     0,    81],\n",
      "        [    0,     0,     0,  ...,     0,     0,     3]]), tensor([154, 325, 674, 136, 299, 750,  23, 941]))\n",
      "torch.Size([8]) torch.Size([461, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([461, 8]) torch.Size([8])\n",
      "(tensor([1, 0, 1, 0, 0, 0, 1, 1]), tensor([[   2,   14,   35,  ...,   13, 4580,  906],\n",
      "        [1965,   21,    7,  ...,  241, 6124,   55],\n",
      "        [   4,   71,    2,  ...,   29,    4,   45],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]]), tensor([157, 362, 135, 220, 461, 353, 148, 209]))\n",
      "torch.Size([8]) torch.Size([291, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([291, 8]) torch.Size([8])\n",
      "(tensor([0, 1, 1, 1, 0, 0, 1, 0]), tensor([[  13,    2, 1824,  ...,    5,    9,  573],\n",
      "        [ 418, 1125,   13,  ...,  329,   39,  456],\n",
      "        [   9,  331,  758,  ...,  218,   38,    7],\n",
      "        ...,\n",
      "        [   0,   11,    0,  ...,    0,    0,    0],\n",
      "        [   0,    3,    0,  ...,    0,    0,    0],\n",
      "        [   0, 1934,    0,  ...,    0,    0,    0]]), tensor([150, 291, 248, 147, 215, 181, 172, 199]))\n",
      "torch.Size([8]) torch.Size([882, 8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([882, 8]) torch.Size([8])\n",
      "(tensor([1, 0, 0, 1, 0, 1, 0, 0]), tensor([[   2,   46,   12,  ...,   13,   14,   13],\n",
      "        [  74,   31, 2607,  ...,  428,   60,   94],\n",
      "        [   6,   65,    7,  ...,    8, 1856,    9],\n",
      "        ...,\n",
      "        [   0,  183,    0,  ...,    0,    0,    0],\n",
      "        [   0,   15,    0,  ...,    0,    0,    0],\n",
      "        [   0,    3,    0,  ...,    0,    0,    0]]), tensor([138, 882, 166,  86, 677, 159, 133, 576]))\n",
      "torch.Size([4]) torch.Size([172, 4]) torch.Size([4])\n",
      "torch.Size([4]) torch.Size([172, 4]) torch.Size([4])\n",
      "(tensor([1, 0, 0, 0]), tensor([[   13,     2,    13,    14],\n",
      "        [  228,   660,    45,   284],\n",
      "        [   14,  3403,  5538,     9],\n",
      "        [   23,     6,  1578,    27],\n",
      "        [   37, 22591,     9,    54],\n",
      "        [    2,    71,    16,    13],\n",
      "        [ 2942,   203,   129,   489],\n",
      "        [ 2070,     8,     4,     8],\n",
      "        [   23,    78,    13,    73],\n",
      "        [ 1340,     6,   267,     3],\n",
      "        [    4,    71,     2,    13],\n",
      "        [    6,    75,   218,  1204],\n",
      "        [   46,   172,    53,    14],\n",
      "        [   17,     8,  1654,    28],\n",
      "        [   29,    89,  6294,   271],\n",
      "        [    5,    20,     4,     6],\n",
      "        [ 2387,     2,    22,   491],\n",
      "        [  831,    74,  1643,     2],\n",
      "        [   12,     3,     9,    21],\n",
      "        [    2,     3,    27,    13],\n",
      "        [  339,     3,    25,   256],\n",
      "        [    3,    11,   246,   545],\n",
      "        [   11,     9,    24,  8704],\n",
      "        [   10,    16,  1417,     6],\n",
      "        [ 1038,   147,    11,   679],\n",
      "        [    8,    13,     8,   138],\n",
      "        [   73,    67,     5,     2],\n",
      "        [   29,    33,   655,    21],\n",
      "        [   70,   233,     3,    19],\n",
      "        [   54,    64,     2,     5],\n",
      "        [    5,  3188,    21,   562],\n",
      "        [   86,   332,    10,     3],\n",
      "        [  397,    12,     5, 12715],\n",
      "        [14456,    23,   121,    54],\n",
      "        [   10,   384,   438,    13],\n",
      "        [    4,     3,    79,   167],\n",
      "        [   22,    60,    13,     9],\n",
      "        [   96,   613,    17,    27],\n",
      "        [ 5493,     3,   991,    45],\n",
      "        [    2,  7667,     3,   462],\n",
      "        [  375,     4,    13,    39],\n",
      "        [    7,     2,    94,    77],\n",
      "        [    2,    74,     9,  1237],\n",
      "        [  705,    47,    27,    37],\n",
      "        [   10,   477,   171,     2],\n",
      "        [  104,     8, 56768,   337],\n",
      "        [    6,  2072,  2496,   294],\n",
      "        [   37,    28,     4,     2],\n",
      "        [  148,    19,    47, 14274],\n",
      "        [    5,    65,   377,    17],\n",
      "        [  196,  1736,     3,    71],\n",
      "        [  610,   289,   615,   660],\n",
      "        [    3,     3,    19,   447],\n",
      "        [   11,     3, 27585,     2],\n",
      "        [   91,     3,  7985,  4368],\n",
      "        [  105,    46,     4,    25],\n",
      "        [   78,    77,    56,   646],\n",
      "        [  110,   104,    57,    24],\n",
      "        [   96,   114,     4,   568],\n",
      "        [ 2005,   190,    56,     8],\n",
      "        [ 5043,    47,   385, 11298],\n",
      "        [   17,  1438,     3,   616],\n",
      "        [    4,    12,    51,     2],\n",
      "        [    6,    46,    26,    88],\n",
      "        [   96,    15,   188,  1609],\n",
      "        [   28,    75,     8,    80],\n",
      "        [  118,   172,   113,    30],\n",
      "        [  363,     8,     5,    17],\n",
      "        [   41,    89,    21,   660],\n",
      "        [ 1397,    20,    15,   595],\n",
      "        [    8,     2,   192,     2],\n",
      "        [ 4115,    74,  2496,   426],\n",
      "        [    6,     4,     4,   498],\n",
      "        [ 4726,    69,    22,    28],\n",
      "        [   12,   166,    10,     2],\n",
      "        [    5,    78,   170,   271],\n",
      "        [ 1166,   240,   159,    17],\n",
      "        [  103,    15,     4,    71],\n",
      "        [    4,     2,    73,    88],\n",
      "        [   39,  1495,   829,    54],\n",
      "        [14975,   705,  1419,    13],\n",
      "        [    3,   167,     9,   125],\n",
      "        [  285,     9,    16,    45],\n",
      "        [   26,    27,     9,   462],\n",
      "        [  703,    71,  2710,     2],\n",
      "        [   39,   127,   203,   212],\n",
      "        [   71,    54, 17033,    40],\n",
      "        [  454,    39,     9,  1572],\n",
      "        [   49,    77,     3, 13539],\n",
      "        [    2,   402,  3442,     3],\n",
      "        [ 2009,     4,     0,    14],\n",
      "        [    6,    48,     0,   212],\n",
      "        [   72,    47,     0,    10],\n",
      "        [  739,    15,     0,    71],\n",
      "        [   37,    11,     0,    86],\n",
      "        [   38,   149,     0,     3],\n",
      "        [    3,    33,     0,    13],\n",
      "        [   11,    85,     0,    33],\n",
      "        [    9,  6819,     0,     2],\n",
      "        [   16,     3,     0, 13872],\n",
      "        [    5,     3,     0,   736],\n",
      "        [  398,     3,     0,    43],\n",
      "        [    6,    69,     0,    14],\n",
      "        [  503,    67,     0,    21],\n",
      "        [ 7213,    33,     0,     3],\n",
      "        [  297,    85,     0,  1023],\n",
      "        [   74,     5,     0,     2],\n",
      "        [    3,  9339,     0,   458],\n",
      "        [    3,     4,     0,   498],\n",
      "        [    3,    29,     0,    10],\n",
      "        [  198,     5,     0,    29],\n",
      "        [  133,   851,     0,    57],\n",
      "        [19273,     3,     0,     4],\n",
      "        [   22,    18,     0,    22],\n",
      "        [   11,    13,     0,    99],\n",
      "        [    9,    33,     0,   577],\n",
      "        [   16,    47,     0,    19],\n",
      "        [   86,   295,     0,    99],\n",
      "        [    8,    38,     0,    68],\n",
      "        [   73,     7,     0,     3],\n",
      "        [   96,     2,     0,   294],\n",
      "        [ 2788,   662,     0,     2],\n",
      "        [    6,    66,     0,    98],\n",
      "        [ 3334,     8,     0,    68],\n",
      "        [ 2009,    14,     0,    76],\n",
      "        [    7,   231,     0,    73],\n",
      "        [  504,   140,     0,     2],\n",
      "        [   58,     2,     0,  4368],\n",
      "        [   34,   511,     0,    39],\n",
      "        [    3,  1297,     0,  4726],\n",
      "        [    0,     3,     0,     5],\n",
      "        [    0,     3,     0,   441],\n",
      "        [    0,     3,     0,     7],\n",
      "        [    0,    13,     0,  4712],\n",
      "        [    0,     9,     0,     3],\n",
      "        [    0,   251,     0,   447],\n",
      "        [    0,    33,     0,   929],\n",
      "        [    0,     8,     0,    10],\n",
      "        [    0,   141,     0,  1951],\n",
      "        [    0,    15,     0,   990],\n",
      "        [    0,    14,     0,    61],\n",
      "        [    0,    17,     0,    30],\n",
      "        [    0,    40,     0,    10],\n",
      "        [    0,   237,     0,    12],\n",
      "        [    0,     2,     0,     2],\n",
      "        [    0,   257,     0,   984],\n",
      "        [    0,     4,     0,     6],\n",
      "        [    0,     6,     0,    10],\n",
      "        [    0,    13,     0,   407],\n",
      "        [    0,    47,     0,   616],\n",
      "        [    0,   489,     0,    15],\n",
      "        [    0,     8,     0,   159],\n",
      "        [    0,  3072,     0,   137],\n",
      "        [    0,   405,     0,    20],\n",
      "        [    0,    29,     0,     2],\n",
      "        [    0,     8,     0, 10416],\n",
      "        [    0,   391,     0,     3],\n",
      "        [    0,    20,     0,   595],\n",
      "        [    0,    14,     0,     2],\n",
      "        [    0,    35,     0,   421],\n",
      "        [    0,     3,     0,    42],\n",
      "        [    0,     0,     0,  1075],\n",
      "        [    0,     0,     0,    14],\n",
      "        [    0,     0,     0,    23],\n",
      "        [    0,     0,     0,    67],\n",
      "        [    0,     0,     0,  2110],\n",
      "        [    0,     0,     0,   847],\n",
      "        [    0,     0,     0,    15],\n",
      "        [    0,     0,     0,   473],\n",
      "        [    0,     0,     0,   222],\n",
      "        [    0,     0,     0,  1991],\n",
      "        [    0,     0,     0,     3]]), tensor([130, 161,  90, 172]))\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i[0].shape, i[1].shape, i[2].shape)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate the performance of model, the frequently adopted solutions are cross validation and the usage of validation set. Since the size of training samples is small, cross validation would be a more appropriate strategy.\n",
    "\n",
    "1. RNN-based model\n",
    "2. Naive-bayes model\n",
    "3. Bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline -TFiDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I watched this film because I'm a big fan of R...\n",
       "1        It does not seem that this movie managed to pl...\n",
       "2              Enough is not a bad movie , just mediocre .\n",
       "3        my friend and i rented this one a few nights a...\n",
       "4        Just about everything in this movie is wrong, ...\n",
       "                               ...                        \n",
       "29336    It 's one of the most honest films ever made a...\n",
       "29337    An absorbing and unsettling psychological drama .\n",
       "29338    Soylent Green IS...a really good movie, actual...\n",
       "29339    There just isn't enough here. There a few funn...\n",
       "29340    This show was absolutely terrible. For one Geo...\n",
       "Name: review, Length: 29341, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=50000, stop_words='english',\n",
       "                token_pattern='(?u)\\\\b[A-Za-z]+\\\\b')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=50000, stop_words='english', token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "vectorizer.fit(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words='english', token_pattern='(?u)\\\\b[A-Za-z]+\\\\b')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "cv.fit(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75046"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, vectorizer, X):\n",
    "    prf = {\n",
    "        'accuracy': list(),\n",
    "        'precision': list(),\n",
    "        'recall': list(),\n",
    "        'f1-score': list()\n",
    "    }\n",
    "\n",
    "    for idx, (train_index, valid_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Cross validation {idx}-fold\")\n",
    "        train, valid = train_df.iloc[train_index], train_df.iloc[valid_index]\n",
    "        X_train, y_train = train['review'], train['sentiment']\n",
    "        X_valid, y_valid = valid['review'], valid['sentiment']\n",
    "\n",
    "        clf = model.fit(vectorizer.transform(X_train), y_train)\n",
    "        X_valid_transform = vectorizer.transform(X_valid)\n",
    "        y_preds = clf.predict(X_valid_transform)\n",
    "        results = precision_recall_fscore_support(y_valid, y_preds, average='binary')\n",
    "        print(results)\n",
    "        prf['accuracy'] += [clf.score(X_valid_transform, y_valid)]\n",
    "        prf['precision'] += [results[0]]\n",
    "        prf['recall'] += [results[1]]\n",
    "        prf['f1-score'] += [results[2]]\n",
    "    return prf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 0-fold\n",
      "(0.855390574564235, 0.8910558170813719, 0.8728590250329381, None)\n",
      "Cross validation 1-fold\n",
      "(0.8485436893203884, 0.8936605316973415, 0.8705179282868525, None)\n",
      "Cross validation 2-fold\n",
      "(0.856957928802589, 0.8951994590939825, 0.8756613756613757, None)\n",
      "Cross validation 3-fold\n",
      "(0.8682519280205655, 0.8947019867549669, 0.8812785388127854, None)\n",
      "Cross validation 4-fold\n",
      "(0.8561335902376365, 0.8940308517773307, 0.8746719160104987, None)\n",
      "Cross validation 5-fold\n",
      "(0.8689384010484927, 0.8822355289421158, 0.8755364806866952, None)\n",
      "Cross validation 6-fold\n",
      "(0.8545688545688546, 0.8835662009314704, 0.868825646058227, None)\n",
      "Cross validation 7-fold\n",
      "(0.8598191214470284, 0.8993243243243243, 0.8791281373844122, None)\n",
      "Cross validation 8-fold\n",
      "(0.86691776522285, 0.9020248203788374, 0.884122919334187, None)\n",
      "Cross validation 9-fold\n",
      "(0.8526785714285714, 0.8895542248835662, 0.8707261478345816, None)\n"
     ]
    }
   ],
   "source": [
    "prf = {\n",
    "    'accuracy': list(),\n",
    "    'precision': list(),\n",
    "    'recall': list(),\n",
    "    'f1-score': list()\n",
    "}\n",
    "\n",
    "for idx, (train_index, valid_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Cross validation {idx}-fold\")\n",
    "    train, valid = train_df.iloc[train_index], train_df.iloc[valid_index]\n",
    "    X_train, y_train = train['review'], train['sentiment']\n",
    "    X_valid, y_valid = valid['review'], valid['sentiment']\n",
    "    \n",
    "    clf = LogisticRegression().fit(vectorizer.transform(X_train), y_train)\n",
    "    X_valid_transform = vectorizer.transform(X_valid)\n",
    "    y_preds = clf.predict(X_valid_transform)\n",
    "    results = precision_recall_fscore_support(y_valid, y_preds, average='binary')\n",
    "    print(results)\n",
    "    prf['accuracy'] += [clf.score(X_valid_transform, y_valid)]\n",
    "    prf['precision'] += [results[0]]\n",
    "    prf['recall'] += [results[1]]\n",
    "    prf['f1-score'] += [results[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8704543802380362\n",
      "precision 0.8588200424661212\n",
      "recall 0.8925353745865309\n",
      "f1-score 0.8753328115102553\n"
     ]
    }
   ],
   "source": [
    "for k, v in prf.items():\n",
    "    print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf + RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 0-fold\n",
      "(0.8378016085790885, 0.8406186953597848, 0.8392077878482713, None)\n",
      "Cross validation 1-fold\n",
      "(0.8282208588957055, 0.8282208588957055, 0.8282208588957055, None)\n",
      "Cross validation 2-fold\n",
      "(0.8199083169613621, 0.8465179175118324, 0.8330006653359947, None)\n",
      "Cross validation 3-fold\n",
      "(0.8421750663129973, 0.8410596026490066, 0.8416169648774022, None)\n",
      "Cross validation 4-fold\n",
      "(0.8337765957446809, 0.8410462776659959, 0.8373956594323873, None)\n",
      "Cross validation 5-fold\n",
      "(0.8387309980171844, 0.844311377245509, 0.8415119363395226, None)\n",
      "Cross validation 6-fold\n",
      "(0.8332212508406187, 0.8243512974051896, 0.82876254180602, None)\n",
      "Cross validation 7-fold\n",
      "(0.8367617783676178, 0.852027027027027, 0.8443254101104787, None)\n",
      "Cross validation 8-fold\n",
      "(0.8389391979301423, 0.8471587197909863, 0.8430289242768931, None)\n",
      "Cross validation 9-fold\n",
      "(0.8310502283105022, 0.8476380572188955, 0.839262187088274, None)\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "prf = train(model, vectorizer, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.833816060079268\n",
      "precision 0.8340585899959899\n",
      "recall 0.8412949830769934\n",
      "f1-score 0.8376332936010948\n"
     ]
    }
   ],
   "source": [
    "for k, v in prf.items():\n",
    "    print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "prf = train(model, vectorizer, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf + NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 0-fold\n",
      "(0.8171281390856407, 0.8533960995292535, 0.8348684210526316, None)\n",
      "Cross validation 1-fold\n",
      "(0.8045977011494253, 0.8588957055214724, 0.830860534124629, None)\n",
      "Cross validation 2-fold\n",
      "(0.8166772756206238, 0.8674780256930358, 0.8413114754098361, None)\n",
      "Cross validation 3-fold\n",
      "(0.8258675078864354, 0.866887417218543, 0.845880452342488, None)\n",
      "Cross validation 4-fold\n",
      "(0.8148148148148148, 0.8558014755197854, 0.8348053647366699, None)\n",
      "Cross validation 5-fold\n",
      "(0.8256765261170548, 0.8729208250166334, 0.8486416558861578, None)\n",
      "Cross validation 6-fold\n",
      "(0.8224358974358974, 0.8536260811709914, 0.8377407770159975, None)\n",
      "Cross validation 7-fold\n",
      "(0.8145569620253165, 0.8695945945945946, 0.8411764705882354, None)\n",
      "Cross validation 8-fold\n",
      "(0.8230529595015577, 0.8628347485303723, 0.8424744897959183, None)\n",
      "Cross validation 9-fold\n",
      "(0.8158730158730159, 0.854956753160346, 0.8349577647823263, None)\n"
     ]
    }
   ],
   "source": [
    "model = BernoulliNB()\n",
    "prf = train(model, vectorizer, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8318395037212776\n",
      "precision 0.8180680799509783\n",
      "recall 0.8616391725955028\n",
      "f1-score 0.8392717405734891\n"
     ]
    }
   ],
   "source": [
    "for k, v in prf.items():\n",
    "    print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = list(zip(train_df['review'], train_df['sentiment']))\n",
    "num_class = len(set([label for (text, label) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, F.one_hot(label, num_classes=num_class).type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_count = 0\n",
    "    all_preds, all_labels = list(), list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            predicted_label = predicted_label.argmax(1)\n",
    "            all_preds += [predicted_label.detach().numpy()]\n",
    "            all_labels += [label.detach().numpy()]\n",
    "            total_count += label.size(0)\n",
    "    print(all_preds, all_labels, total_count)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    prf = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    print(prf)\n",
    "    return (all_preds == all_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassificationModel(\n",
      "  (embedding): EmbeddingBag(102971, 64, mode=mean)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Cross validation 0-fold\n",
      "[array([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1]), array([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0])] [array([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0])] 100\n",
      "(0.6933333333333334, 1.0, 0.8188976377952756, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.45s | valid accuracy    0.770 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]), array([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0])] [array([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]), array([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1])] 100\n",
      "(1.0, 0.5961538461538461, 0.7469879518072289, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.49s | valid accuracy    0.790 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0]), array([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])] [array([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "       0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1]), array([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1])] 100\n",
      "(0.972972972972973, 0.6923076923076923, 0.8089887640449438, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.46s | valid accuracy    0.830 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 1-fold\n",
      "[array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0]), array([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1])] [array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0]), array([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1])] 100\n",
      "(0.9423076923076923, 1.0, 0.9702970297029703, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.970 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]), array([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1])] [array([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]), array([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0])] 100\n",
      "(0.875, 1.0, 0.9333333333333333, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.43s | valid accuracy    0.930 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0]), array([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "       0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0])] [array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0]), array([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "       0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0])] 100\n",
      "(0.9245283018867925, 1.0, 0.9607843137254902, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.47s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 2-fold\n",
      "[array([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1]), array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1])] [array([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1]), array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1])] 100\n",
      "(0.9245283018867925, 0.98, 0.9514563106796116, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]), array([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "       0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1])] [array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]), array([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "       0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1])] 100\n",
      "(0.9245283018867925, 0.98, 0.9514563106796116, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1]), array([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "       1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1])] [array([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1]), array([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1])] 100\n",
      "(0.9245283018867925, 0.98, 0.9514563106796116, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.46s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 3-fold\n",
      "[array([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0]), array([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0])] [array([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0]), array([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0])] 100\n",
      "(0.9433962264150944, 0.9803921568627451, 0.9615384615384616, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]), array([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "       1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1])] [array([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]), array([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "       1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1])] 100\n",
      "(0.9433962264150944, 0.9803921568627451, 0.9615384615384616, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.44s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]), array([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0])] [array([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]), array([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0])] 100\n",
      "(0.9433962264150944, 0.9803921568627451, 0.9615384615384616, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.45s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 4-fold\n",
      "[array([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0]), array([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1])] [array([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "       0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0]), array([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1])] 100\n",
      "(0.9534883720930233, 0.9318181818181818, 0.942528735632184, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1]), array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0])] [array([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1]), array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0])] 100\n",
      "(0.9534883720930233, 0.9318181818181818, 0.942528735632184, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]), array([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0])] [array([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0])] 100\n",
      "(0.9534883720930233, 0.9318181818181818, 0.942528735632184, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.45s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 5-fold\n",
      "[array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]), array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0])] [array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]), array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0])] 100\n",
      "(0.9285714285714286, 0.9512195121951219, 0.9397590361445782, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1]), array([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0])] [array([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]), array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0])] 100\n",
      "(0.9285714285714286, 0.9512195121951219, 0.9397590361445782, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.45s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]), array([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0])] [array([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]), array([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0])] 100\n",
      "(0.9285714285714286, 0.9512195121951219, 0.9397590361445782, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 6-fold\n",
      "[array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0]), array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1])] [array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0]), array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "       1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1])] 100\n",
      "(0.9824561403508771, 0.9655172413793104, 0.9739130434782608, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.45s | valid accuracy    0.970 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]), array([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "       0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0])] [array([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]), array([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0])] 100\n",
      "(0.9824561403508771, 0.9655172413793104, 0.9739130434782608, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.43s | valid accuracy    0.970 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1]), array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "       0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0])] [array([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1]), array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "       0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0])] 100\n",
      "(0.9824561403508771, 0.9655172413793104, 0.9739130434782608, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.45s | valid accuracy    0.970 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 7-fold\n",
      "[array([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1]), array([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1])] [array([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1]), array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1])] 100\n",
      "(1.0, 0.9107142857142857, 0.9532710280373832, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.43s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1])] [array([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1])] 100\n",
      "(1.0, 0.9107142857142857, 0.9532710280373832, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.43s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]), array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0])] [array([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]), array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0])] 100\n",
      "(1.0, 0.9107142857142857, 0.9532710280373832, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.42s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 8-fold\n",
      "[array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1]), array([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "       1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0])] [array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1]), array([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0])] 100\n",
      "(0.9591836734693877, 0.9591836734693877, 0.9591836734693877, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.45s | valid accuracy    0.960 \n",
      "-----------------------------------------------------------\n",
      "[array([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]), array([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1])] [array([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]), array([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1])] 100\n",
      "(0.94, 0.9591836734693877, 0.9494949494949495, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.46s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]), array([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1])] [array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]), array([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1])] 100\n",
      "(0.94, 0.9591836734693877, 0.9494949494949495, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.44s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 9-fold\n",
      "[array([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]), array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1])] [array([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0]), array([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1])] 100\n",
      "(0.9607843137254902, 0.9423076923076923, 0.9514563106796117, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.45s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]), array([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1])] [array([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0]), array([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1])] 100\n",
      "(0.9607843137254902, 0.9423076923076923, 0.9514563106796117, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.45s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[array([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]), array([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0])] [array([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]), array([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0])] 100\n",
      "(0.9607843137254902, 0.9423076923076923, 0.9514563106796117, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.49s | valid accuracy    0.950 \n",
      "-----------------------------------------------------------\n",
      "[0.83, 0.97, 0.95, 0.96, 0.95, 0.95, 0.97, 0.95, 0.96, 0.95]\n",
      "0.9439999999999997\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "# Hyperparameters\n",
    "EPOCHS = 1 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# train_iter = list(zip(train_df.iloc[:1000]['review'], train_df.iloc[:1000]['sentiment']))\n",
    "\n",
    "X, y = train_df.iloc[:1000]['review'], train_df['sentiment'].iloc[:1000].to_numpy()\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# train_dataset = list(train_iter)\n",
    "# test_dataset = list(test_iter)\n",
    "# num_train = int(len(train_dataset) * 0.95)\n",
    "# split_train_, split_valid_ = \\\n",
    "#     random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "# train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "#                               shuffle=True, collate_fn=collate_batch)\n",
    "# valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "#                               shuffle=True, collate_fn=collate_batch)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "#                              shuffle=True, collate_fn=collate_batch)\n",
    "print(model)\n",
    "total_acc = []\n",
    "for idx, (train_index, valid_index) in enumerate(kf.split(X)):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(f\"Cross validation {idx}-fold\")\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "    train_iter = list(zip(X_train, y_train))\n",
    "    valid_iter = list(zip(X_valid, y_valid))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "    valid_dataloader = DataLoader(valid_iter, batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True, collate_fn=collate_batch)\n",
    "    cross_acc = None\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_dataloader)\n",
    "        acc = evaluate(valid_dataloader)\n",
    "        if cross_acc is not None and cross_acc > acc:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            cross_acc = acc\n",
    "        print('-' * 59)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "              'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                               time.time() - epoch_start_time,\n",
    "                                                acc))\n",
    "        print('-' * 59)\n",
    "    total_acc += [cross_acc]\n",
    "print(total_acc)\n",
    "print(np.mean(total_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RNN-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    # target is hidden_size\n",
    "    def __init__(self, hidden_size, method='concat'):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        if method not in ('dot', 'general', 'concat'):\n",
    "            raise NotImplemented\n",
    "        if method == 'general':\n",
    "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        elif method == 'concat':\n",
    "            self.attn = nn.Linear(2 * hidden_size, hidden_size)\n",
    "            self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        if hasattr(self, 'attn'):\n",
    "            self.attn.weight.data.uniform_(-initrange, initrange)\n",
    "            self.attn.bias.data.zero_()\n",
    "        if hasattr(self, 'v'):\n",
    "            self.v.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.matmul(hidden, encoder_output)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        attn = self.attn(encoder_output)\n",
    "        return torch.matmul(hidden, attn)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        hidden_reshape = torch.unsqueeze(hidden, dim=0).repeat(encoder_output.size(0), 1, 1)\n",
    "        attn = self.attn(torch.cat([hidden_reshape, encoder_output], dim=-1)).tanh()\n",
    "        return self.v(attn).squeeze(dim=-1)\n",
    "\n",
    "    def forward(self, hidden, encoder_output):\n",
    "        # output = [lengths x batch_size x hidden_size]\n",
    "        # hidden = [batch_size x hidden_size]\n",
    "        attn_scores = None\n",
    "        if self.method == 'dot':\n",
    "            attn_scores = self.dot_score(hidden, encoder_output)\n",
    "        elif self.method == 'general':\n",
    "            attn_scores = self.general_score(hidden, encoder_output)\n",
    "        elif self.method == 'concat':\n",
    "            attn_scores = self.concat_score(hidden, encoder_output)\n",
    "\n",
    "        # [lengths x batch_size] -> [batch_size x lengths]\n",
    "        attn_scores = attn_scores.t()\n",
    "        # return [batch_size x 1 x lengths]\n",
    "        return F.softmax(attn_scores, dim=-1).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 5, 3, 2, 1])\n",
      "tensor([5, 3, 7, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([5,3,7,2,1])\n",
    "sorted_t, idx = t.sort(descending=True)\n",
    "print(sorted_t)\n",
    "print(torch.gather(t, 0, torch.arange(0, idx.shape[0], dtype=torch.int64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "def sort_sequence(inputs, lengths):\n",
    "    sorted_lengths, sorted_idx = lengths.sort(descending=True)\n",
    "    return inputs[sorted_idx], sorted_lengths, sorted_idx\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, n_layers, dropout, num_classes, attention_mode, padding_idx=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, sparse=True, padding_idx=1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, n_layers, dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        self.attn = Attention(2 * hidden_size, attention_mode)\n",
    "        self.fc = nn.Linear(2 * hidden_size, num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        for param in self.lstm.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.uniform_(param.data, -initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, text_lengths, hidden=None):\n",
    "        sorted_lengths, sorted_idx = text_lengths.sort(descending=True)\n",
    "        sorted_text = torch.index_select(text, -1, sorted_idx)\n",
    "        emb = self.embedding(sorted_text)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, sorted_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        hidden_state, _ = hidden\n",
    "        hidden_state = hidden_state[-2:,:,:].view(1, -1, 2 * hidden_size).squeeze(0)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        attn_weights = self.attn(hidden_state, outputs)\n",
    "        # attn_weights = [batch_size x 1 x lengths]\n",
    "        context = torch.bmm(attn_weights, outputs.transpose(0, 1)).squeeze(1)\n",
    "        output = self.fc(context)\n",
    "        output = torch.index_select(output, 0, torch.arange(0, sorted_idx.shape[0], dtype=torch.int64))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50\n",
    "hidden_size = 256\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "num_classes = 2\n",
    "attention_mode = 'concat'\n",
    "model = LSTMModel(vocab_size, embed_size, hidden_size, n_layers, \n",
    "                  dropout, num_classes, attention_mode).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label, _ = model(text, offsets)\n",
    "        loss = criterion(predited_label, F.one_hot(label, num_classes=num_class).type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_count = 0\n",
    "    all_preds, all_labels = list(), list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label, _ = model(text, offsets)\n",
    "            predicted_label = predicted_label.argmax(1)\n",
    "            all_preds += [predicted_label.detach().numpy()]\n",
    "            all_labels += [label.detach().numpy()]\n",
    "            total_count += label.size(0)\n",
    "    print(all_preds, all_labels, total_count)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    prf = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    print(prf)\n",
    "    return (all_preds == all_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embedding): Embedding(102971, 50, padding_idx=1, sparse=True)\n",
      "  (lstm): LSTM(50, 256, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (attn): Attention(\n",
      "    (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "Cross validation 0-fold\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([4, 2])\n",
      "[array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0])] [array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 0, 0, 0, 1]), array([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "       1, 1, 0, 0, 1, 0, 1, 1, 1, 1]), array([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 0, 1, 0]), array([1, 0, 0, 1])] 100\n",
      "(0.5, 0.038461538461538464, 0.07142857142857144, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 1770.09s | valid accuracy    0.480 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 1-fold\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([4, 2])\n",
      "[array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0])] [array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       1, 1, 1, 0, 1, 0, 0, 1, 0, 1]), array([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 1, 0, 1, 0, 0, 0, 1, 1]), array([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1]), array([0, 1, 1, 1])] 100\n",
      "(0.5, 0.04081632653061224, 0.07547169811320753, None)\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 1824.12s | valid accuracy    0.510 \n",
      "-----------------------------------------------------------\n",
      "Cross validation 2-fold\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "# Hyperparameters\n",
    "EPOCHS = 1 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 32 # batch size for training\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "X, y = train_df.iloc[:1000]['review'], train_df['sentiment'].iloc[:1000].to_numpy()\n",
    "print(model)\n",
    "total_acc = []\n",
    "for idx, (train_index, valid_index) in enumerate(kf.split(X)):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(f\"Cross validation {idx}-fold\")\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "    train_iter = list(zip(X_train, y_train))\n",
    "    valid_iter = list(zip(X_valid, y_valid))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "    valid_dataloader = DataLoader(valid_iter, batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True, collate_fn=collate_batch)\n",
    "    cross_acc = None\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_dataloader)\n",
    "        acc = evaluate(valid_dataloader)\n",
    "        if cross_acc is not None and cross_acc > acc:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            cross_acc = acc\n",
    "        print('-' * 59)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "              'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                               time.time() - epoch_start_time,\n",
    "                                                acc))\n",
    "        print('-' * 59)\n",
    "    total_acc += [cross_acc]\n",
    "print(total_acc)\n",
    "print(np.mean(total_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4545a62de3741b5917f48d7da05bb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  I am a good person, and how about you?\n",
      "Tokenized:  ['i', 'am', 'a', 'good', 'person', ',', 'and', 'how', 'about', 'you', '?']\n",
      "Token IDs:  [101, 1045, 2572, 1037, 2204, 2711, 1010, 1998, 2129, 2055, 2017, 1029, 100]\n"
     ]
    }
   ],
   "source": [
    "text = \"I am a good person, and how about you?\"\n",
    "print(' Original: ', text)\n",
    "print('Tokenized: ', tokenizer.tokenize(text))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokenizer.tokenize(text)+ [\"SEP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2572, 1037, 2204, 2711, 1010, 1998, 2129, 2055, 2017, 1029, 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/hlu/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1938: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2572, 1037, 2204, 2711, 1010, 1998, 2129, 2055, 2017, 1029,\n",
       "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\n",
    "                text, add_special_tokens=True, max_length=64,\n",
    "                pad_to_max_length=True, return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bec40be47d84561a3bd779be34845ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a9022900a040238fe3486506b041ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "from transformers import XLNetModel, XLNetForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = XLNetModel.from_pretrained('xlnet-base-cased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetModel(\n",
       "  (word_embedding): Embedding(32000, 768)\n",
       "  (layer): ModuleList(\n",
       "    (0): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am a good boy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', 'i', '▁am', '▁a', '▁good', '▁boy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict = tokenizer.encode_plus(\n",
    "    \"<cls> asd11l2js I am a good boy\", add_special_tokens=True, max_length=50,\n",
    "    pad_to_max_length=True, return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "            5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "            5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    3,   34,\n",
       "           66, 1545,  368,  184, 1315,   23,   17,  150,  569,   24,  195, 2001,\n",
       "            4,    3]]), 'token_type_ids': tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 2]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list, attention_masks = list(), list()\n",
    "text_list.append(encoded_dict['input_ids'])\n",
    "attention_masks.append(encoded_dict['attention_mask'])\n",
    "text_list = torch.cat(text_list, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(text_list, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetModelOutput(last_hidden_state=tensor([[[ 1.4764,  2.5515, -0.8054,  ..., -0.7408,  0.5854, -2.2659],\n",
       "         [ 1.6141,  2.4739, -0.7761,  ..., -0.7215,  0.4234, -2.3880],\n",
       "         [ 1.5958,  2.4999, -0.7793,  ..., -0.6517,  0.2706, -2.4049],\n",
       "         ...,\n",
       "         [ 3.8463,  0.2613, -1.5299,  ..., -2.6709, -0.2188,  0.2802],\n",
       "         [ 5.7440, -0.2734, -3.4873,  ..., -3.3266,  0.4171,  0.4994],\n",
       "         [ 3.7563, -0.2951, -2.6874,  ..., -1.8446,  0.5208,  0.9403]]],\n",
       "       grad_fn=<PermuteBackward>), mems=(tensor([[[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252]],\n",
       "\n",
       "        [[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252]],\n",
       "\n",
       "        [[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0540,  0.0375,  0.0332,  ...,  0.0201, -0.0480,  0.0724]],\n",
       "\n",
       "        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n",
       "\n",
       "        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[ 0.9156,  1.0634, -0.2741,  ..., -0.1662, -0.1363, -0.8428]],\n",
       "\n",
       "        [[ 0.9461,  1.0958, -0.2464,  ..., -0.1756, -0.1703, -0.8440]],\n",
       "\n",
       "        [[ 0.9758,  1.1076, -0.2352,  ..., -0.1836, -0.1847, -0.8192]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.8328,  0.5979,  0.6304,  ..., -0.4727, -1.2793,  1.6664]],\n",
       "\n",
       "        [[ 1.0250, -0.4521, -1.5119,  ..., -0.0787,  0.4344,  0.7233]],\n",
       "\n",
       "        [[ 0.1938,  0.1017, -2.3535,  ..., -0.5091, -0.1052,  1.6627]]]), tensor([[[ 0.3933,  0.3043, -0.2023,  ...,  0.1985,  0.2346, -0.5402]],\n",
       "\n",
       "        [[ 0.4375,  0.3185, -0.1855,  ...,  0.1863,  0.1941, -0.5373]],\n",
       "\n",
       "        [[ 0.4548,  0.3255, -0.1658,  ...,  0.1743,  0.1989, -0.5231]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0949,  0.1630,  0.7266,  ...,  0.1444, -1.3562,  1.3974]],\n",
       "\n",
       "        [[ 0.3575, -0.6885, -1.4503,  ..., -0.9342,  0.5952,  0.3722]],\n",
       "\n",
       "        [[-0.0512, -0.6750, -2.2458,  ..., -0.7432,  0.3301,  1.2587]]]), tensor([[[ 0.2071,  0.9489, -0.6494,  ...,  0.9213, -0.2973, -0.9567]],\n",
       "\n",
       "        [[ 0.2374,  0.9585, -0.6248,  ...,  0.9006, -0.3489, -0.9238]],\n",
       "\n",
       "        [[ 0.2464,  0.9809, -0.6202,  ...,  0.8855, -0.3586, -0.8963]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7476,  0.6471,  0.5672,  ..., -0.3418, -1.2544,  0.5400]],\n",
       "\n",
       "        [[-0.1788,  0.2359, -1.5412,  ..., -0.8350,  0.2809, -0.4816]],\n",
       "\n",
       "        [[-0.2697, -0.3103, -2.6733,  ..., -0.4990, -0.2893,  0.5538]]]), tensor([[[ 1.4693,  0.0391, -0.7497,  ...,  1.5123, -0.5623, -0.5832]],\n",
       "\n",
       "        [[ 1.5013,  0.0470, -0.7449,  ...,  1.5048, -0.6189, -0.5611]],\n",
       "\n",
       "        [[ 1.5103,  0.0562, -0.7554,  ...,  1.4908, -0.6479, -0.5522]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5714, -0.2590,  0.1343,  ...,  0.4249, -0.8150,  0.3106]],\n",
       "\n",
       "        [[ 0.5622, -0.4188, -1.8813,  ..., -0.0362, -0.3020, -0.0747]],\n",
       "\n",
       "        [[ 0.7146, -0.7559, -2.6028,  ...,  0.1587, -0.5205,  0.1761]]]), tensor([[[ 1.3464,  0.4357, -0.9102,  ...,  0.5386, -1.3498, -1.5001]],\n",
       "\n",
       "        [[ 1.3908,  0.4392, -0.9228,  ...,  0.5474, -1.3786, -1.4790]],\n",
       "\n",
       "        [[ 1.4049,  0.4384, -0.9358,  ...,  0.5331, -1.3844, -1.4777]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.3468,  0.6591,  0.5007,  ..., -0.4910, -0.7752, -0.1183]],\n",
       "\n",
       "        [[ 0.3819,  0.3082, -1.4197,  ...,  0.0685, -0.3215, -0.7804]],\n",
       "\n",
       "        [[ 0.5596, -0.0037, -2.2351,  ...,  0.0812, -0.5723, -0.3554]]]), tensor([[[ 1.0619,  0.6954, -2.0038,  ..., -0.3962, -1.9340, -1.0415]],\n",
       "\n",
       "        [[ 1.1375,  0.7019, -1.9848,  ..., -0.3725, -2.0207, -0.9980]],\n",
       "\n",
       "        [[ 1.2100,  0.6881, -1.9637,  ..., -0.3556, -2.0325, -1.0247]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.4129,  0.9158,  0.6443,  ..., -0.9858, -1.9156,  0.5450]],\n",
       "\n",
       "        [[ 0.5878,  0.1117, -1.9039,  ..., -0.2968, -0.6944, -0.0989]],\n",
       "\n",
       "        [[ 0.0981, -0.2018, -2.8803,  ..., -0.3664, -0.9444,  0.1085]]]), tensor([[[ 0.6015,  1.1536, -1.6290,  ..., -0.6842, -1.4619, -0.5704]],\n",
       "\n",
       "        [[ 0.6530,  1.1733, -1.6237,  ..., -0.6925, -1.5167, -0.5520]],\n",
       "\n",
       "        [[ 0.6745,  1.1747, -1.6395,  ..., -0.6756, -1.5211, -0.5644]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.9452,  1.1977,  1.3949,  ..., -1.0470, -0.7192,  1.2919]],\n",
       "\n",
       "        [[ 0.7428,  1.1699, -1.1035,  ..., -1.0125, -0.3954,  0.8313]],\n",
       "\n",
       "        [[-0.2416,  1.0022, -1.9961,  ..., -0.9663, -0.4888,  0.9290]]]), tensor([[[ 0.8313,  1.3704, -1.9395,  ..., -0.0134, -1.8234, -0.5403]],\n",
       "\n",
       "        [[ 0.9062,  1.3974, -1.9473,  ..., -0.0270, -1.8912, -0.4990]],\n",
       "\n",
       "        [[ 0.9857,  1.4109, -1.9736,  ..., -0.0311, -1.9189, -0.5002]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5132,  1.5491,  0.5664,  ..., -0.8778, -1.2237,  0.7145]],\n",
       "\n",
       "        [[ 1.4038,  1.0113, -1.2193,  ..., -1.1647, -0.5987, -0.3652]],\n",
       "\n",
       "        [[-0.1021,  1.2665, -2.0035,  ..., -0.8056, -0.1584, -0.0238]]]), tensor([[[ 0.6594,  0.8182, -2.0167,  ..., -0.5913, -0.6711, -0.3155]],\n",
       "\n",
       "        [[ 0.7234,  0.7991, -1.9943,  ..., -0.6506, -0.7660, -0.2838]],\n",
       "\n",
       "        [[ 0.7489,  0.8035, -2.0283,  ..., -0.6751, -0.8198, -0.2729]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7829,  0.5609,  0.3888,  ..., -1.3396, -0.7099,  1.1600]],\n",
       "\n",
       "        [[ 0.9701,  0.0308, -1.4273,  ..., -1.8822,  0.3705, -0.0501]],\n",
       "\n",
       "        [[ 0.0121,  0.6457, -1.9682,  ..., -0.9920,  0.6946,  0.0159]]]), tensor([[[ 0.7801,  0.9696, -2.0823,  ..., -0.8309, -0.3148,  0.0946]],\n",
       "\n",
       "        [[ 0.8911,  0.9265, -2.0851,  ..., -0.8720, -0.3838,  0.0760]],\n",
       "\n",
       "        [[ 0.9231,  0.9462, -2.1496,  ..., -0.8830, -0.4642,  0.0740]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0418,  0.4412,  0.4693,  ..., -1.4395, -1.1413,  1.8067]],\n",
       "\n",
       "        [[ 1.3913,  0.0687, -1.5188,  ..., -2.0519, -0.1029,  1.2681]],\n",
       "\n",
       "        [[ 0.3388,  0.8382, -1.4421,  ..., -0.9915,  0.1114,  1.0360]]]), tensor([[[ 0.5958,  0.9143, -0.8302,  ..., -0.0795,  0.0356,  0.3544]],\n",
       "\n",
       "        [[ 0.6664,  0.8663, -0.8206,  ..., -0.0660, -0.0228,  0.3231]],\n",
       "\n",
       "        [[ 0.6616,  0.8829, -0.8320,  ..., -0.0452, -0.0994,  0.2967]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3205, -0.6818, -0.0454,  ..., -0.6727, -0.7787,  1.1092]],\n",
       "\n",
       "        [[ 1.3218, -0.7020, -1.0760,  ..., -0.9728, -0.2405,  0.8265]],\n",
       "\n",
       "        [[ 0.2434, -0.3605, -0.8424,  ..., -0.0908, -0.1475,  0.6281]]])), hidden_states=(tensor([[[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "         [ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "         [ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "         ...,\n",
       "         [ 0.0540,  0.0375,  0.0332,  ...,  0.0201, -0.0480,  0.0724],\n",
       "         [ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520],\n",
       "         [ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.9156,  1.0634, -0.2741,  ..., -0.1662, -0.1363, -0.8428],\n",
       "         [ 0.9461,  1.0958, -0.2464,  ..., -0.1756, -0.1703, -0.8440],\n",
       "         [ 0.9758,  1.1076, -0.2352,  ..., -0.1836, -0.1847, -0.8192],\n",
       "         ...,\n",
       "         [ 0.8328,  0.5979,  0.6304,  ..., -0.4727, -1.2793,  1.6664],\n",
       "         [ 1.0250, -0.4521, -1.5119,  ..., -0.0787,  0.4344,  0.7233],\n",
       "         [ 0.1938,  0.1017, -2.3535,  ..., -0.5091, -0.1052,  1.6627]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.3933,  0.3043, -0.2023,  ...,  0.1985,  0.2346, -0.5402],\n",
       "         [ 0.4375,  0.3185, -0.1855,  ...,  0.1863,  0.1941, -0.5373],\n",
       "         [ 0.4548,  0.3255, -0.1658,  ...,  0.1743,  0.1989, -0.5231],\n",
       "         ...,\n",
       "         [-0.0949,  0.1630,  0.7266,  ...,  0.1444, -1.3562,  1.3974],\n",
       "         [ 0.3575, -0.6885, -1.4503,  ..., -0.9342,  0.5952,  0.3722],\n",
       "         [-0.0512, -0.6750, -2.2458,  ..., -0.7432,  0.3301,  1.2587]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.2071,  0.9489, -0.6494,  ...,  0.9213, -0.2973, -0.9567],\n",
       "         [ 0.2374,  0.9585, -0.6248,  ...,  0.9006, -0.3489, -0.9238],\n",
       "         [ 0.2464,  0.9809, -0.6202,  ...,  0.8855, -0.3586, -0.8963],\n",
       "         ...,\n",
       "         [-0.7476,  0.6471,  0.5672,  ..., -0.3418, -1.2544,  0.5400],\n",
       "         [-0.1788,  0.2359, -1.5412,  ..., -0.8350,  0.2809, -0.4816],\n",
       "         [-0.2697, -0.3103, -2.6733,  ..., -0.4990, -0.2893,  0.5538]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 1.4693,  0.0391, -0.7497,  ...,  1.5123, -0.5623, -0.5832],\n",
       "         [ 1.5013,  0.0470, -0.7449,  ...,  1.5048, -0.6189, -0.5611],\n",
       "         [ 1.5103,  0.0562, -0.7554,  ...,  1.4908, -0.6479, -0.5522],\n",
       "         ...,\n",
       "         [-0.5714, -0.2590,  0.1343,  ...,  0.4249, -0.8150,  0.3106],\n",
       "         [ 0.5622, -0.4188, -1.8813,  ..., -0.0362, -0.3020, -0.0747],\n",
       "         [ 0.7146, -0.7559, -2.6028,  ...,  0.1587, -0.5205,  0.1761]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 1.3464,  0.4357, -0.9102,  ...,  0.5386, -1.3498, -1.5001],\n",
       "         [ 1.3908,  0.4392, -0.9228,  ...,  0.5474, -1.3786, -1.4790],\n",
       "         [ 1.4049,  0.4384, -0.9358,  ...,  0.5331, -1.3844, -1.4777],\n",
       "         ...,\n",
       "         [-1.3468,  0.6591,  0.5007,  ..., -0.4910, -0.7752, -0.1183],\n",
       "         [ 0.3819,  0.3082, -1.4197,  ...,  0.0685, -0.3215, -0.7804],\n",
       "         [ 0.5596, -0.0037, -2.2351,  ...,  0.0812, -0.5723, -0.3554]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 1.0619,  0.6954, -2.0038,  ..., -0.3962, -1.9340, -1.0415],\n",
       "         [ 1.1375,  0.7019, -1.9848,  ..., -0.3725, -2.0207, -0.9980],\n",
       "         [ 1.2100,  0.6881, -1.9637,  ..., -0.3556, -2.0325, -1.0247],\n",
       "         ...,\n",
       "         [-1.4129,  0.9158,  0.6443,  ..., -0.9858, -1.9156,  0.5450],\n",
       "         [ 0.5878,  0.1117, -1.9039,  ..., -0.2968, -0.6944, -0.0989],\n",
       "         [ 0.0981, -0.2018, -2.8803,  ..., -0.3664, -0.9444,  0.1085]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.6015,  1.1536, -1.6290,  ..., -0.6842, -1.4619, -0.5704],\n",
       "         [ 0.6530,  1.1733, -1.6237,  ..., -0.6925, -1.5167, -0.5520],\n",
       "         [ 0.6745,  1.1747, -1.6395,  ..., -0.6756, -1.5211, -0.5644],\n",
       "         ...,\n",
       "         [-0.9452,  1.1977,  1.3949,  ..., -1.0470, -0.7192,  1.2919],\n",
       "         [ 0.7428,  1.1699, -1.1035,  ..., -1.0125, -0.3954,  0.8313],\n",
       "         [-0.2416,  1.0022, -1.9961,  ..., -0.9663, -0.4888,  0.9290]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.8313,  1.3704, -1.9395,  ..., -0.0134, -1.8234, -0.5403],\n",
       "         [ 0.9062,  1.3974, -1.9473,  ..., -0.0270, -1.8912, -0.4990],\n",
       "         [ 0.9857,  1.4109, -1.9736,  ..., -0.0311, -1.9189, -0.5002],\n",
       "         ...,\n",
       "         [-0.5132,  1.5491,  0.5664,  ..., -0.8778, -1.2237,  0.7145],\n",
       "         [ 1.4038,  1.0113, -1.2193,  ..., -1.1647, -0.5987, -0.3652],\n",
       "         [-0.1021,  1.2665, -2.0035,  ..., -0.8056, -0.1584, -0.0238]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.6594,  0.8182, -2.0167,  ..., -0.5913, -0.6711, -0.3155],\n",
       "         [ 0.7234,  0.7991, -1.9943,  ..., -0.6506, -0.7660, -0.2838],\n",
       "         [ 0.7489,  0.8035, -2.0283,  ..., -0.6751, -0.8198, -0.2729],\n",
       "         ...,\n",
       "         [-0.7829,  0.5609,  0.3888,  ..., -1.3396, -0.7099,  1.1600],\n",
       "         [ 0.9701,  0.0308, -1.4273,  ..., -1.8822,  0.3705, -0.0501],\n",
       "         [ 0.0121,  0.6457, -1.9682,  ..., -0.9920,  0.6946,  0.0159]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.7801,  0.9696, -2.0823,  ..., -0.8309, -0.3148,  0.0946],\n",
       "         [ 0.8911,  0.9265, -2.0851,  ..., -0.8720, -0.3838,  0.0760],\n",
       "         [ 0.9231,  0.9462, -2.1496,  ..., -0.8830, -0.4642,  0.0740],\n",
       "         ...,\n",
       "         [-0.0418,  0.4412,  0.4693,  ..., -1.4395, -1.1413,  1.8067],\n",
       "         [ 1.3913,  0.0687, -1.5188,  ..., -2.0519, -0.1029,  1.2681],\n",
       "         [ 0.3388,  0.8382, -1.4421,  ..., -0.9915,  0.1114,  1.0360]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 0.5958,  0.9143, -0.8302,  ..., -0.0795,  0.0356,  0.3544],\n",
       "         [ 0.6664,  0.8663, -0.8206,  ..., -0.0660, -0.0228,  0.3231],\n",
       "         [ 0.6616,  0.8829, -0.8320,  ..., -0.0452, -0.0994,  0.2967],\n",
       "         ...,\n",
       "         [ 0.3205, -0.6818, -0.0454,  ..., -0.6727, -0.7787,  1.1092],\n",
       "         [ 1.3218, -0.7020, -1.0760,  ..., -0.9728, -0.2405,  0.8265],\n",
       "         [ 0.2434, -0.3605, -0.8424,  ..., -0.0908, -0.1475,  0.6281]]],\n",
       "       grad_fn=<PermuteBackward>), tensor([[[ 1.4764,  2.5515, -0.8054,  ..., -0.7408,  0.5854, -2.2659],\n",
       "         [ 1.6141,  2.4739, -0.7761,  ..., -0.7215,  0.4234, -2.3880],\n",
       "         [ 1.5958,  2.4999, -0.7793,  ..., -0.6517,  0.2706, -2.4049],\n",
       "         ...,\n",
       "         [ 3.8463,  0.2613, -1.5299,  ..., -2.6709, -0.2188,  0.2802],\n",
       "         [ 5.7440, -0.2734, -3.4873,  ..., -3.3266,  0.4171,  0.4994],\n",
       "         [ 3.7563, -0.2951, -2.6874,  ..., -1.8446,  0.5208,  0.9403]]],\n",
       "       grad_fn=<PermuteBackward>)), attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "          [ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "          [ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n",
       "          ...,\n",
       "          [ 0.0540,  0.0375,  0.0332,  ...,  0.0201, -0.0480,  0.0724],\n",
       "          [ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520],\n",
       "          [ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.9156,  1.0634, -0.2741,  ..., -0.1662, -0.1363, -0.8428],\n",
       "          [ 0.9461,  1.0958, -0.2464,  ..., -0.1756, -0.1703, -0.8440],\n",
       "          [ 0.9758,  1.1076, -0.2352,  ..., -0.1836, -0.1847, -0.8192],\n",
       "          ...,\n",
       "          [ 0.8328,  0.5979,  0.6304,  ..., -0.4727, -1.2793,  1.6664],\n",
       "          [ 1.0250, -0.4521, -1.5119,  ..., -0.0787,  0.4344,  0.7233],\n",
       "          [ 0.1938,  0.1017, -2.3535,  ..., -0.5091, -0.1052,  1.6627]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.3933,  0.3043, -0.2023,  ...,  0.1985,  0.2346, -0.5402],\n",
       "          [ 0.4375,  0.3185, -0.1855,  ...,  0.1863,  0.1941, -0.5373],\n",
       "          [ 0.4548,  0.3255, -0.1658,  ...,  0.1743,  0.1989, -0.5231],\n",
       "          ...,\n",
       "          [-0.0949,  0.1630,  0.7266,  ...,  0.1444, -1.3562,  1.3974],\n",
       "          [ 0.3575, -0.6885, -1.4503,  ..., -0.9342,  0.5952,  0.3722],\n",
       "          [-0.0512, -0.6750, -2.2458,  ..., -0.7432,  0.3301,  1.2587]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.2071,  0.9489, -0.6494,  ...,  0.9213, -0.2973, -0.9567],\n",
       "          [ 0.2374,  0.9585, -0.6248,  ...,  0.9006, -0.3489, -0.9238],\n",
       "          [ 0.2464,  0.9809, -0.6202,  ...,  0.8855, -0.3586, -0.8963],\n",
       "          ...,\n",
       "          [-0.7476,  0.6471,  0.5672,  ..., -0.3418, -1.2544,  0.5400],\n",
       "          [-0.1788,  0.2359, -1.5412,  ..., -0.8350,  0.2809, -0.4816],\n",
       "          [-0.2697, -0.3103, -2.6733,  ..., -0.4990, -0.2893,  0.5538]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 1.4693,  0.0391, -0.7497,  ...,  1.5123, -0.5623, -0.5832],\n",
       "          [ 1.5013,  0.0470, -0.7449,  ...,  1.5048, -0.6189, -0.5611],\n",
       "          [ 1.5103,  0.0562, -0.7554,  ...,  1.4908, -0.6479, -0.5522],\n",
       "          ...,\n",
       "          [-0.5714, -0.2590,  0.1343,  ...,  0.4249, -0.8150,  0.3106],\n",
       "          [ 0.5622, -0.4188, -1.8813,  ..., -0.0362, -0.3020, -0.0747],\n",
       "          [ 0.7146, -0.7559, -2.6028,  ...,  0.1587, -0.5205,  0.1761]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 1.3464,  0.4357, -0.9102,  ...,  0.5386, -1.3498, -1.5001],\n",
       "          [ 1.3908,  0.4392, -0.9228,  ...,  0.5474, -1.3786, -1.4790],\n",
       "          [ 1.4049,  0.4384, -0.9358,  ...,  0.5331, -1.3844, -1.4777],\n",
       "          ...,\n",
       "          [-1.3468,  0.6591,  0.5007,  ..., -0.4910, -0.7752, -0.1183],\n",
       "          [ 0.3819,  0.3082, -1.4197,  ...,  0.0685, -0.3215, -0.7804],\n",
       "          [ 0.5596, -0.0037, -2.2351,  ...,  0.0812, -0.5723, -0.3554]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 1.0619,  0.6954, -2.0038,  ..., -0.3962, -1.9340, -1.0415],\n",
       "          [ 1.1375,  0.7019, -1.9848,  ..., -0.3725, -2.0207, -0.9980],\n",
       "          [ 1.2100,  0.6881, -1.9637,  ..., -0.3556, -2.0325, -1.0247],\n",
       "          ...,\n",
       "          [-1.4129,  0.9158,  0.6443,  ..., -0.9858, -1.9156,  0.5450],\n",
       "          [ 0.5878,  0.1117, -1.9039,  ..., -0.2968, -0.6944, -0.0989],\n",
       "          [ 0.0981, -0.2018, -2.8803,  ..., -0.3664, -0.9444,  0.1085]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.6015,  1.1536, -1.6290,  ..., -0.6842, -1.4619, -0.5704],\n",
       "          [ 0.6530,  1.1733, -1.6237,  ..., -0.6925, -1.5167, -0.5520],\n",
       "          [ 0.6745,  1.1747, -1.6395,  ..., -0.6756, -1.5211, -0.5644],\n",
       "          ...,\n",
       "          [-0.9452,  1.1977,  1.3949,  ..., -1.0470, -0.7192,  1.2919],\n",
       "          [ 0.7428,  1.1699, -1.1035,  ..., -1.0125, -0.3954,  0.8313],\n",
       "          [-0.2416,  1.0022, -1.9961,  ..., -0.9663, -0.4888,  0.9290]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.8313,  1.3704, -1.9395,  ..., -0.0134, -1.8234, -0.5403],\n",
       "          [ 0.9062,  1.3974, -1.9473,  ..., -0.0270, -1.8912, -0.4990],\n",
       "          [ 0.9857,  1.4109, -1.9736,  ..., -0.0311, -1.9189, -0.5002],\n",
       "          ...,\n",
       "          [-0.5132,  1.5491,  0.5664,  ..., -0.8778, -1.2237,  0.7145],\n",
       "          [ 1.4038,  1.0113, -1.2193,  ..., -1.1647, -0.5987, -0.3652],\n",
       "          [-0.1021,  1.2665, -2.0035,  ..., -0.8056, -0.1584, -0.0238]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.6594,  0.8182, -2.0167,  ..., -0.5913, -0.6711, -0.3155],\n",
       "          [ 0.7234,  0.7991, -1.9943,  ..., -0.6506, -0.7660, -0.2838],\n",
       "          [ 0.7489,  0.8035, -2.0283,  ..., -0.6751, -0.8198, -0.2729],\n",
       "          ...,\n",
       "          [-0.7829,  0.5609,  0.3888,  ..., -1.3396, -0.7099,  1.1600],\n",
       "          [ 0.9701,  0.0308, -1.4273,  ..., -1.8822,  0.3705, -0.0501],\n",
       "          [ 0.0121,  0.6457, -1.9682,  ..., -0.9920,  0.6946,  0.0159]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.7801,  0.9696, -2.0823,  ..., -0.8309, -0.3148,  0.0946],\n",
       "          [ 0.8911,  0.9265, -2.0851,  ..., -0.8720, -0.3838,  0.0760],\n",
       "          [ 0.9231,  0.9462, -2.1496,  ..., -0.8830, -0.4642,  0.0740],\n",
       "          ...,\n",
       "          [-0.0418,  0.4412,  0.4693,  ..., -1.4395, -1.1413,  1.8067],\n",
       "          [ 1.3913,  0.0687, -1.5188,  ..., -2.0519, -0.1029,  1.2681],\n",
       "          [ 0.3388,  0.8382, -1.4421,  ..., -0.9915,  0.1114,  1.0360]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 0.5958,  0.9143, -0.8302,  ..., -0.0795,  0.0356,  0.3544],\n",
       "          [ 0.6664,  0.8663, -0.8206,  ..., -0.0660, -0.0228,  0.3231],\n",
       "          [ 0.6616,  0.8829, -0.8320,  ..., -0.0452, -0.0994,  0.2967],\n",
       "          ...,\n",
       "          [ 0.3205, -0.6818, -0.0454,  ..., -0.6727, -0.7787,  1.1092],\n",
       "          [ 1.3218, -0.7020, -1.0760,  ..., -0.9728, -0.2405,  0.8265],\n",
       "          [ 0.2434, -0.3605, -0.8424,  ..., -0.0908, -0.1475,  0.6281]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[ 1.4764,  2.5515, -0.8054,  ..., -0.7408,  0.5854, -2.2659],\n",
       "          [ 1.6141,  2.4739, -0.7761,  ..., -0.7215,  0.4234, -2.3880],\n",
       "          [ 1.5958,  2.4999, -0.7793,  ..., -0.6517,  0.2706, -2.4049],\n",
       "          ...,\n",
       "          [ 3.8463,  0.2613, -1.5299,  ..., -2.6709, -0.2188,  0.2802],\n",
       "          [ 5.7440, -0.2734, -3.4873,  ..., -3.3266,  0.4171,  0.4994],\n",
       "          [ 3.7563, -0.2951, -2.6874,  ..., -1.8446,  0.5208,  0.9403]]],\n",
       "        grad_fn=<PermuteBackward>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
